{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJIYfMbgZ0Go"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_IWBcrGnKm8",
        "outputId": "3f152864-ec42-4bb7-aeba-c8a80b9b8e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydriller in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from pydriller) (3.1.42)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from pydriller) (2023.4)\n",
            "Requirement already satisfied: types-pytz in /usr/local/lib/python3.10/dist-packages (from pydriller) (2024.1.0.20240203)\n",
            "Requirement already satisfied: lizard in /usr/local/lib/python3.10/dist-packages (from pydriller) (1.17.10)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->pydriller) (4.0.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->pydriller) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydriller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn5Ut4fWFX8v",
        "outputId": "195966c8-c10e-4af4-9180-f8f7d805afd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFrBL6Dz1d63",
        "outputId": "c7dd4b9d-28a9-408e-d1a7-103daa0e3d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  6753  100  6753    0     0   8831      0 --:--:-- --:--:-- --:--:--  8831\n",
            "This script will download and install the Joern tools on your machine. Proceed? [Y/n]: ^C\n"
          ]
        }
      ],
      "source": [
        "# !curl -L \"https://github.com/joernio/joern/releases/latest/download/joern-install.sh\" -o joern-install.sh\n",
        "# !chmod u+x joern-install.sh\n",
        "# !echo -e \"Y\\n\\nY\\n\\nY\\n\" | ./joern-install.sh --interactive\n",
        "\n",
        "!curl -L \"https://github.com/joernio/joern/releases/latest/download/joern-install.sh\" -o joern-install.sh\n",
        "!chmod u+x joern-install.sh\n",
        "!./joern-install.sh --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcgSRMTaLoRZ"
      },
      "source": [
        "# TreeMaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEByd-_H2grt"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "class TreeMaker:\n",
        "    def __init__(self, FILE_PATH_bf, FILE_PATH_af, OUT_bf, OUT_af,Format):\n",
        "        self.FILE_PATH_bf = FILE_PATH_bf\n",
        "        self.FILE_PATH_af = FILE_PATH_af\n",
        "        self.OUT_bf = OUT_bf\n",
        "        self.OUT_af = OUT_af\n",
        "        self.Format = Format\n",
        "\n",
        "    def generate_AST_trees(self):\n",
        "        self.generate_trees(\"ast\")\n",
        "\n",
        "    def generate_CFG_trees(self):\n",
        "        self.generate_trees(\"cfg\")\n",
        "\n",
        "    def generate_CDG_trees(self):\n",
        "        self.generate_trees(\"cdg\")\n",
        "\n",
        "    def generate_DDG_trees(self):\n",
        "        self.generate_trees(\"ddg\")\n",
        "\n",
        "    def generate_PDG_trees(self):\n",
        "        self.generate_trees(\"pdg\")\n",
        "\n",
        "    def generate_CPG_trees(self):\n",
        "        self.generate_trees(\"cpg14\")\n",
        "    def generate_ALL_trees(self):\n",
        "        self.generate_trees(\"all\")\n",
        "\n",
        "    # def generate_trees(self, tree_type):\n",
        "    #     commands = [\n",
        "    #         f\"joern-parse {self.FILE_PATH_bf}\",\n",
        "    #         f\"mkdir -p {self.OUT_bf}\",\n",
        "    #         f\"rm -r {self.OUT_bf}/{tree_type}\",\n",
        "    #         f\"joern-export --repr={tree_type} --format={self.Format} --out {self.OUT_bf}/{tree_type}\",\n",
        "    #         f\"joern-parse {self.FILE_PATH_af}\",\n",
        "    #         f\"mkdir -p {self.OUT_af}\",\n",
        "    #         f\"rm -r {self.OUT_af}/{tree_type}\",\n",
        "    #         f\"joern-export --repr={tree_type} --format={self.Format} --out {self.OUT_af}/{tree_type}\"\n",
        "    #     ]\n",
        "\n",
        "    #     for cmd in commands:\n",
        "    #         process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "    #         output, error = process.communicate()\n",
        "\n",
        "    #         if error:\n",
        "    #             print(\"Error:\", error.decode())\n",
        "    #         else:\n",
        "    #             print(\"Output:\", output.decode())\n",
        "\n",
        "\n",
        "    def generate_trees(self, tree_type):\n",
        "        commands = [\n",
        "            f\"joern-parse {self.FILE_PATH_bf}\",\n",
        "            f\"mkdir -p {self.OUT_bf}\",\n",
        "            f\"joern-parse {self.FILE_PATH_af}\",\n",
        "            f\"mkdir -p {self.OUT_af}\",\n",
        "        ]\n",
        "\n",
        "        for cmd in commands:\n",
        "            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "            output, error = process.communicate()\n",
        "\n",
        "            if error:\n",
        "                print(\"Error:\", error.decode())\n",
        "            else:\n",
        "                print(\"Output:\", output.decode())\n",
        "\n",
        "        # Check if the output directory exists before attempting to remove it\n",
        "        if os.path.exists(f\"{self.OUT_bf}/{tree_type}\"):\n",
        "            rm_command = f\"rm -r {self.OUT_bf}/{tree_type}\"\n",
        "            process = subprocess.Popen(rm_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "            _, error = process.communicate()\n",
        "\n",
        "            if error:\n",
        "                print(\"Error:\", error.decode())\n",
        "\n",
        "        if os.path.exists(f\"{self.OUT_af}/{tree_type}\"):\n",
        "            rm_command = f\"rm -r {self.OUT_af}/{tree_type}\"\n",
        "            process = subprocess.Popen(rm_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "            _, error = process.communicate()\n",
        "\n",
        "            if error:\n",
        "                print(\"Error:\", error.decode())\n",
        "\n",
        "        export_commands = [\n",
        "            f\"joern-export --repr={tree_type} --format={self.Format} --out {self.OUT_bf}/{tree_type}\",\n",
        "            f\"joern-export --repr={tree_type} --format={self.Format} --out {self.OUT_af}/{tree_type}\"\n",
        "        ]\n",
        "\n",
        "        for export_cmd in export_commands:\n",
        "            process = subprocess.Popen(export_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "            output, error = process.communicate()\n",
        "\n",
        "            if error:\n",
        "                print(\"Error:\", error.decode())\n",
        "            else:\n",
        "                print(\"Output:\", output.decode())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-9clOj2LrLV"
      },
      "source": [
        "# Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JqT8mNNQi0Bh"
      },
      "outputs": [],
      "source": [
        "shared_prompt = \"\"\"You are an expert programmer, and you are trying to summarize a git diff.\n",
        "                    Reminders about the git diff format:\n",
        "                    For every file, there are a few metadata lines, like (for example):\n",
        "                    \\\\\\`\n",
        "                    diff --git a/lib/index.js b/lib/index.js\n",
        "                    index aadf691..bfef603 100644\n",
        "                    --- a/lib/index.js\n",
        "                    +++ b/lib/index.js\n",
        "                    \\\\\\`\n",
        "                    This means that \\lib/index.js\\ was modified in this commit. Note that this is only an example.\n",
        "                    Then there is a specifier of the lines that were modified.\n",
        "                    A line starting with \\+\\ means it was added.\n",
        "                    A line that starting with \\-\\ means that line was deleted.\n",
        "                    A line that starts with neither \\+\\ nor \\-\\ is code given for context and better understanding.\n",
        "                    It is not part of the diff.\n",
        "                    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i9mSNmxqFS_E"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "import requests\n",
        "\n",
        "class SummaryGenerator_Gemini:\n",
        "\n",
        "  def extract_contents(self, fpath):\n",
        "    with open(fpath, 'r') as file:\n",
        "      contents = file.read()\n",
        "    return contents\n",
        "\n",
        "  def promt_response(self, prompt, graph_before, graph_after, graph_type):\n",
        "    response = model.generate_content(prompt +\" \"+graph_type+\"  of code before commit : \"+graph_before+\"  \"+graph_type + \" of code after commit : \"+ graph_after)\n",
        "    return response.text\n",
        "\n",
        "  def ast_summaries(self, issue_title = None):\n",
        "    #prompt = \"You are given a two Abstract Syntax Trees of some programming language code snippet before and after a commit. I need to know what both the code snippets do exactly. Explain briefly in few lines just the functinoality of both the codes. Also tell what was the change and how did it change the functionality of the code and what issue did it solve in the code.\"\n",
        "    prompt = \"You are given a two Abstract Syntax Trees of some programming language code snippet before and after a commit. Explain briefly what was the change and how did it change the functionality of the code and if possible what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/ast/1-ast.dot\"\n",
        "    method_after=\"/content/method_after/ast/1-ast.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"AST\")\n",
        "\n",
        "    print(\"Summary based on Ast: \", summary)\n",
        "    return summary\n",
        "\n",
        "  def cdg_summaries(self, issue_title = None):\n",
        "    prompt = \"You are given a two Control Dependence graphs of some programming language code snippet before and after a commit. Summarize control dependence of both code snippets. Explain briefly in few lines how did the commit change the code and what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/cdg/1-cdg.dot\"\n",
        "    method_after=\"/content/method_after/cdg/1-cdg.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"CDG\")\n",
        "\n",
        "    print(\"Summary based on Cdg: \", summary)\n",
        "    return summary\n",
        "\n",
        "  def cfg_summaries(self, issue_title = None):\n",
        "    prompt = \"You are given a two Control Flow graphs of some programming language code snippet before and after a commit. I need to know control flow of both the graphs. Explain briefly in few lines how did it change the functionality or the control flow of the code and what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/cfg/1-cfg.dot\"\n",
        "    method_after=\"/content/method_after/cfg/1-cfg.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"CFG\")\n",
        "\n",
        "    print(\"Summary based on Cfg: \", summary)\n",
        "    return summary\n",
        "\n",
        "  def pdg_summeries(self, issue_title = None):\n",
        "    prompt = \"You are given a two Program dependence graphs of some programming language code snippet before and after a commit. I need to know if there is any dependency changes. Explain briefly in few lines how did it change the functionality of the code and what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/pdg/1-pdg.dot\"\n",
        "    method_after=\"/content/method_after/pdg/1-pdg.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"PDG\")\n",
        "\n",
        "    print(\"Summary based on Pdg: \", summary)\n",
        "    return summary\n",
        "\n",
        "  def cpg_summaries(self, issue_title = None):\n",
        "    prompt = \"You are given two code property graphs of some programming language code snippet before and after a commit. I need to know what both the code snippets do exactly. Explain briefly in few lines just the functinoality of both the codes. Also tell what was the change and how did it change the functionality of the code and what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/cpg14/1-cpg.dot\"\n",
        "    method_after=\"/content/method_after/cpg14/1-cpg.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"CPG\")\n",
        "\n",
        "    print(\"Summary based on Cpg: \", summary)\n",
        "    return summary\n",
        "\n",
        "  def all_summaries(self, issue_title = None):\n",
        "    prompt = \"You are given two property graphs of some programming language code snippet before and after a commit. I need to know what both the code snippets do exactly. Explain briefly in few lines just the functinoality of both the codes. Also tell what was the change and how did it change the functionality of the code and what issue did it solve in the code.\"\n",
        "\n",
        "    if(issue_title):\n",
        "      prompt = prompt+ \" This commit is linked to the issue \"+issue_title\n",
        "\n",
        "    method_before=\"/content/method_before/all/export.dot\"\n",
        "    method_after=\"/content/method_after/all/export.dot\"\n",
        "\n",
        "    digraph_before = self.extract_contents(method_before)\n",
        "    digraph_after = self.extract_contents(method_after)\n",
        "\n",
        "    summary = self.promt_response(prompt,digraph_before,digraph_after, \"All\")\n",
        "\n",
        "    # print(\"Summary based on All: \", summary)\n",
        "\n",
        "    return summary\n",
        "\n",
        "  def commit_summary(self, method_wise_summaries, issue_title = None):\n",
        "\n",
        "    prompt =     \"\"\"\n",
        "    I have a commit in which some number of methods changed. Summarise the commit and tell me what type of commit it might be from the following categories like \"Bugfix\", \"Feature\", \"Dependencies\", \"Platform Support\".\n",
        "    Here \"Bugfix\" means anything related to fixing bugs or errors in the code.\n",
        "    \"Feature\" means anything related to increasing functionality or features in the software\n",
        "    \"Dependencies\" means anything related to solving current dependency issues\n",
        "    \"Platform Support\" means anything related to increasing usability of the software on different platforms like linux and windows or ios and android\"\n",
        "    \"\"\"\n",
        "    if(issue_title):\n",
        "      prompt+= \"The commit is linked to the issue titled, \"+ issue_title\n",
        "\n",
        "    prompt+= \"Below I am giving you the summaries of different methods that got changed.\"\n",
        "\n",
        "    for single_method_sumamry in method_wise_summaries:\n",
        "      prompt+= single_method_sumamry\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    commit_summary = response.text\n",
        "    return commit_summary\n",
        "\n",
        "  def method_summary(self,method_bf,method_af):\n",
        "    if(method_bf==None ):\n",
        "      method_bf=\"\"\n",
        "    if(method_af==None ):\n",
        "      method_af=\"\"\n",
        "    prompt= \"\"\"\" I have two versions of method code before and after a commit , give me key differences as concisely as possible in software engineer point of view, method before : \"\"\"\n",
        "    prompt=prompt+method_bf+\"\\n\"+\"method after: \"+method_af\n",
        "    response = model.generate_content(prompt)\n",
        "    method_summary = response.text\n",
        "    # print(f\"\\n\\n method: \\n {method_summary} \")\n",
        "    return method_summary\n",
        "\n",
        "  def file_summary(self,fileName,file_bf,file_af,all_modified_methods_summaries):\n",
        "    prompt= \"\"\"\" I have two versions of file code before and after a commit and summaries of changes in the file's methods , give me key differences as concisely as possible in software engineer point of view, file before : \"\"\"\n",
        "    prompt=prompt+file_bf+\"\\n\"+\"file after: \"+file_af\n",
        "    for f,methodName,method_summary in all_modified_methods_summaries:\n",
        "      prompt = prompt+\"\\n\"+methodName+\" change summary : \\n\"+method_summary\n",
        "    response = model.generate_content(prompt)\n",
        "    file_summary = response.text\n",
        "    # print(f\"\\n\\n\\n file {fileName} : \\n {file_summary} \")\n",
        "    return file_summary\n",
        "\n",
        "  def file_summary_individual(self,fileName,file_bf,file_af):\n",
        "        prompt= \"\"\"\" I have two versions of file code before and after a commit.\n",
        "          File before commit:\n",
        "          \"\"\"+file_bf+\"\"\"\n",
        "\n",
        "          File after commit:\n",
        "          \"\"\"+file_af+\"\"\"\n",
        "\n",
        "          Please summarize it in a comment, describing the changes made in the diff in high level.\n",
        "          Do it in the following way:\n",
        "          Write the file name and then write a brief summary of changes of what happend in the file. Keep it as simple as possible like headline which can help a software engineer understand easily. \"\"\"\n",
        "        prompt=prompt+file_bf+\"\\n\"+\"file after: \"+file_af\n",
        "        # for f,methodName,method_summary in all_modified_methods_summaries:\n",
        "        #   prompt = prompt+\"\\n\"+methodName+\" change summary : \\n\"+method_summary\n",
        "        response = model.generate_content(prompt)\n",
        "        file_summary = response.text\n",
        "        # print(f\"\\n\\n\\n file: \\n \")\n",
        "        return file_summary\n",
        "\n",
        "  def file_summary_fdiff(self,fileName,file_diff):\n",
        "      # prompt= \"\"\"\" I have two versions of file code before and after a commit , give me key differences as concisely as possible in software engineer point of view, file before : \"\"\"\n",
        "      prompt=\"\"\"You are an expert programmer, and you are trying to summarize a git diff.\n",
        "        Reminders about the git diff format:\n",
        "        For every file, there are a few metadata lines, like (for example):\n",
        "        \\\\\\`\n",
        "        diff --git a/lib/index.js b/lib/index.js\n",
        "        index aadf691..bfef603 100644\n",
        "        --- a/lib/index.js\n",
        "        +++ b/lib/index.js\n",
        "        \\\\\\`\n",
        "        This means that \\lib/index.js\\ was modified in this commit. Note that this is only an example.\n",
        "        Then there is a specifier of the lines that were modified.\n",
        "        A line starting with \\+\\ means it was added.\n",
        "        A line that starting with \\-\\ means that line was deleted.\n",
        "        A line that starts with neither \\+\\ nor \\-\\ is code given for context and better understanding.\n",
        "        It is not part of the diff.\n",
        "\n",
        "        The following is a git diff of a single file. \"\"\"+file_diff+\"\"\"\n",
        "        Please summarize it in a comment, describing the changes made in the diff in high level.\n",
        "        Do it in the following way:\n",
        "        Write the file name and then write a brief summary of changes of what happend in the file. Keep it as simple as possible like headline which can help a software engineer understand easily.\"\"\"\n",
        "      # for f,methodName,method_summary in all_modified_methods_summaries:\n",
        "      #   prompt = prompt+\"\\n\"+methodName+\" change summary : \\n\"+method_summary\n",
        "      response = model.generate_content(prompt)\n",
        "      file_summary = response.text\n",
        "      # print(f\"\\n\\n\\n file: \\n \")\n",
        "      return file_summary\n",
        "\n",
        "  # def commit_summary_code(self,commit_msg,file_summaries):\n",
        "  #   prompt= \"\"\"\"\n",
        "  #   I have file change summaries before and after a commit and commit message , give me key differences as concisely as possible in software engineer point of view,\n",
        "  #   Summarise the commit and tell me what type of commit it might be from the following categories like \"Bugfix\", \"Feature\", \"Dependencies\", \"Platform Support\".\n",
        "  #    Here \"Bugfix\" means anything related to fixing bugs or errors in the code.\n",
        "  #    \"Feature\" means anything related to increasing functionality or features in the software\n",
        "  #    \"Dependencies\" means anything related to solving current dependency issues\n",
        "  #    \"Platform Support\" means anything related to increasing usability of the software on different platforms like linux and windows or ios and android\"\n",
        "\n",
        "  #   \"\"\"\n",
        "  #   # prompt = \"\"\"\n",
        "  #   # I have a commit in which some number of methods changed. Summarise the commit and tell me what type of commit it might be from the following categories like \"Bugfix\", \"Feature\", \"Dependencies\", \"Platform Support\".\n",
        "  #   # Here \"Bugfix\" means anything related to fixing bugs or errors in the code.\n",
        "  #   # \"Feature\" means anything related to increasing functionality or features in the software\n",
        "  #   # \"Dependencies\" means anything related to solving current dependency issues\n",
        "  #   # \"Platform Support\" means anything related to increasing usability of the software on different platforms like linux and windows or ios and android\"\n",
        "  #   # \"\"\"\n",
        "  #   for fileName , file_summary in file_summaries:\n",
        "  #     prompt= prompt + \"\\nIn this file\" + fileName + \" the changes have been summarized like this : \\n\"+file_summary\n",
        "  #   prompt=prompt+\"\\n Commit message : \"+commit_msg+\" don't give seperated file summaries\"\n",
        "  #   # print(prompt)\n",
        "  #   response = model.generate_content(prompt)\n",
        "  #   commit_summary = response.text\n",
        "  #   # print(\"Commit summary: \", commit_summary)\n",
        "  #   # print(f\"\\n\\n\\n\\n\\n\\n commit : \\n {commit_summary} \")\n",
        "  #   return commit_summary\n",
        "\n",
        "  def commit_summary_code_diff(self,commit_msg,file_summaries,FILE_SUMMARIES_BASED_ON_RAW_GIT_DIFF):\n",
        "    file_summary=\"\"\n",
        "    file_summaries_diff=\"\"\n",
        "    for fn,fs in file_summaries:\n",
        "      file_summary=file_summary+fn+\" :\\n\"+fs+\"\\n\"\n",
        "    for fn,fs in FILE_SUMMARIES_BASED_ON_RAW_GIT_DIFF:\n",
        "      file_summaries_diff=file_summaries_diff+fn+\" :\\n\"+fs+\"\\n\"\n",
        "    prompt=\"\"\"\n",
        "      In a given commmit, the summaries of changes in files generated based on before and after files are gives as\\n\"\"\"+file_summary+\"\"\"and the summaries based on only the raw git diffs is gives as\\n\"\"\"+file_summaries_diff+\"\"\".\n",
        "      The commit message given by the user for this commit is : \"\"\"+commit_msg+\"\"\".\n",
        "      Now based on both the summaries, on a high level summaries the overall commit into one or multiple of following categories in software engineer point of view. \n",
        "      For exaple if a commit is of the type bug fix and dependencies, give only two lines one for the bug fix and one for the tests, with bug fix and tests between dollar sign like $bug fix$ and $tests$ with a very small and brief explnation for each\n",
        "\n",
        "      Given below is a small explanation for each category:\n",
        "\n",
        "      Feature: Commits related to implementing new features or functionalities in the project from the user perspective rather than extra classes or methods in code.\n",
        "\n",
        "      Bug Fix: Commits addressing and fixing issues, bugs, or defects in the codebase.\n",
        "\n",
        "      Refactor: Commits focused on improving code quality, organization, or structure without changing its external behavior.\n",
        "\n",
        "      Documentation: Commits related to updating or adding documentation such as README files, inline comments, or documentation files only\n",
        "\n",
        "      Performance: Commits aimed at improving the performance of the codebase, such as optimizing algorithms, reducing resource usage, or enhancing execution speed.\n",
        "\n",
        "      Dependency Update: Commits updating dependencies, importing any new libraries, or third-party components used in the project to newer versions.\n",
        "\n",
        "      Tests: Commits related to adding, updating, or fixing tests to ensure code quality and reliability.\n",
        "\n",
        "      Configuration: Commits updating configuration files, such as environment variables, build scripts, or project settings.\n",
        "\n",
        "      Localization/Internationalization: Commits related to adding or updating translations, localization, or internationalization features in the project.\n",
        "\n",
        "      Style/Formatting: Commits focused on enforcing consistent code style, formatting, or coding conventions across the codebase.\n",
        "\n",
        "      Chore: Commits related to general maintenance tasks, administrative work, or other miscellaneous changes that don't fit into other categories.\n",
        "\n",
        "      Security: Commits addressing security vulnerabilities, implementing security measures, or ensuring compliance with security standards.\"\"\"\n",
        "    # print(prompt)\n",
        "    response = model.generate_content(prompt)\n",
        "    commit_summary = response.text\n",
        "    # print(\"Commit summary: \", commit_summary)\n",
        "    # print(f\"\\n\\n\\n\\n\\n\\n commit : \\n {commit_summary} \")\n",
        "    return commit_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "SAv1-IhmF1jX",
        "outputId": "a8fb74ae-b7e0-40fb-cc39-879473fc3d5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Definition:**\\nLife is a complex and dynamic process characterized by organization, energy utilization, growth, response to stimuli, reproduction, and adaptability. It is the fundamental property that distinguishes living beings from non-living matter.\\n\\n**Scientific Perspective:**\\n\\n* **Cellular Structure:** All life is composed of cells, the basic units of life. Cells contain genetic material (DNA) that provides instructions for cellular processes.\\n* **Metabolism:** Living organisms constantly exchange energy and materials with their surroundings. Metabolism includes processes like respiration, photosynthesis, and digestion.\\n* **Reproduction:** Living organisms can reproduce, creating offspring that share their genetic traits. This ensures the continuation of species.\\n* **Homeostasis:** Living organisms maintain a stable internal environment (e.g., temperature, pH) through various regulatory mechanisms.\\n* **Adaptation:** Living organisms evolve over time, developing traits that enhance their survival and reproduction in their specific environment.\\n\\n**Philosophical Perspective:**\\n\\n* **Emergence:** Life is an emergent property that arises from the interactions of non-living molecules and systems.\\n* **Purpose:** The purpose of life is debated among philosophers. Some argue for intrinsic meaning, while others emphasize the absence of inherent purpose.\\n* **Consciousness:** Many living organisms exhibit consciousness, the ability to perceive and experience the world.\\n* **Value:** The value of life is highly subjective and influenced by cultural, religious, and personal beliefs.\\n\\n**Other Conceptualizations:**\\n\\n* **Thermodynamic Perspective:** Life is seen as a pattern of energy flow and organization that opposes the tendency towards entropy (disorder).\\n* **Cybernetic Perspective:** Life is characterized by self-regulation and information processing, similar to cybernetic systems.\\n* **Systems Theory:** Life is viewed as a complex system that interacts with its environment and is capable of self-organization and adaptability.\\n\\n**Key Aspects:**\\n\\n* Life exhibits a wide range of complexity, from single-celled organisms to complex multicellular beings.\\n* Life is characterized by a constant exchange of energy and matter with the environment.\\n* Life has the capacity to change and adapt over time in response to environmental challenges.\\n* The origin of life is a complex and ongoing area of scientific investigation.'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#genai.configure(api_key='AIzaSyBPes3Qot8ZqLb7hm1je4kcbSdK6B1SXCU')\n",
        "# genai.configure(api_key='AIzaSyBB0bRxLfGj6sunPywGHcHFjfqO3W6qAjk')\n",
        "genai.configure(api_key='AIzaSyB_xXjlHgJ7I6_0vH4MCILeu6IBQTb9soU')\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(\"What is life?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj-b-dyzLt5T"
      },
      "source": [
        "# GitHub Info Fetcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rk6I8mknNfu",
        "outputId": "0056e57f-c779-42fe-c227-9efa89f1fd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"pull_request_id\": 144,\n",
            "        \"issue_number\": \"143\",\n",
            "        \"issue_title\": \"OpenSourceApp needs openai keys\",\n",
            "        \"commits_sha\": [\n",
            "            \"fd0c44b91363455f5a3d156a9500d14ed87bde92\"\n",
            "        ],\n",
            "        \"merge_commit_sha\": \"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import base64\n",
        "\n",
        "class GitHubInfoFetcher:\n",
        "    def __init__(self, token):\n",
        "        self.token = token\n",
        "        self.base_url = \"https://api.github.com\"\n",
        "        self.headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    def get_issue_body(self, owner, repo, issue_number):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/issues/{issue_number}\"\n",
        "        headers = {'Authorization': f'token {self.token}'}\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            issue_details = response.json()\n",
        "            issue_title = issue_details.get(\"title\")\n",
        "            return issue_title\n",
        "        else:\n",
        "            print(f\"Failed to fetch issue body for issue {issue_number}: {response.status_code}\")\n",
        "            print(\"Response body:\", response.text)\n",
        "            return None\n",
        "\n",
        "    def get_pr_commits(self, owner, repo, pull_request_id):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pull_request_id}/commits\"\n",
        "        headers = {'Authorization': f'token {self.token}'}\n",
        "        commits_sha = []\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            commits = response.json()\n",
        "            for commit in commits :\n",
        "                commits_sha.append(commit.get('sha'))\n",
        "\n",
        "            return commits_sha\n",
        "        else:\n",
        "            print(f\"Failed to fetch commits for pull request {pull_request_id}: {response.status_code}\")\n",
        "            print(\"Response body:\", response.text)\n",
        "            return None\n",
        "\n",
        "    def get_issue_number_from_pull_request(self, pr):\n",
        "        pull_request_body = pr.get('body')\n",
        "        if pull_request_body:\n",
        "            match = re.search(r'(?:fix(?:es)?(?:ed)?|resolve(?:d)?|close(?:s)?(?:d)?|solve(?:d)?)[^\\S\\r\\n]*:?\\s*(https://github\\.com/[^/]+/[^/]+/issues/(\\d+))', pull_request_body, re.IGNORECASE)\n",
        "            if match:\n",
        "                issue_number = match.group(2)\n",
        "                return issue_number\n",
        "            else:\n",
        "                return \"No issue reference found in the pull request body.\\n\"\n",
        "\n",
        "    def get_merged_pull_requests(self, owner, repo, since):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/pulls?state=closed\"\n",
        "        headers = {'Authorization': f'token {self.token}'}\n",
        "        pr_issue_numbers = []\n",
        "\n",
        "        params = {'per_page': 100}  # Adjust per_page as needed\n",
        "        page = 1\n",
        "        edge_pr = None\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url + f\"&page={page}\", headers=headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                pull_requests = response.json()\n",
        "                if not pull_requests:  # No more pull requests left\n",
        "                    break\n",
        "\n",
        "                last_pr = pull_requests[-1]\n",
        "\n",
        "                merged_at_str = last_pr.get(\"merged_at\")\n",
        "                if merged_at_str:\n",
        "                    merged_at = datetime.strptime(merged_at_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                else:\n",
        "                    merged_at = None\n",
        "\n",
        "                 # Stop fetching if last pull request is before 'since' date\n",
        "\n",
        "                for pr in pull_requests:\n",
        "                    if pr.get(\"merged_at\"):\n",
        "                        pr_number = pr.get(\"number\")\n",
        "                        merged_at = datetime.strptime(pr.get(\"merged_at\"), \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                        # print(pr_number,\" \", merged_at)\n",
        "                        if merged_at >= since:\n",
        "                            issue_number = self.get_issue_number_from_pull_request(pr)\n",
        "                            # print(issue_number)\n",
        "                            if issue_number and issue_number.isdigit():\n",
        "                                #print(pr_number, \" \", merged_at)\n",
        "                                # print(\"Complete\",pr)\n",
        "                                pr_issue_numbers.append({'pull_request_id': pr_number, 'issue_number': issue_number})\n",
        "                        else:\n",
        "                            edge_pr = pr  # Update edge pull request\n",
        "\n",
        "                page += 1\n",
        "                if merged_at < since or merged_at is None:\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Failed to fetch merged pull requests: {response.status_code}\")\n",
        "                print(\"Response body:\", response.text)\n",
        "                break\n",
        "\n",
        "        return pr_issue_numbers\n",
        "\n",
        "    def get_all_merged_pull_requests(self, owner, repo, since):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/pulls?state=closed\"\n",
        "        headers = {'Authorization': f'token {self.token}'}\n",
        "        pr_issue_numbers = []\n",
        "\n",
        "        params = {'per_page': 100}  # Adjust per_page as needed\n",
        "        page = 1\n",
        "        edge_pr = None\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url + f\"&page={page}\", headers=headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                pull_requests = response.json()\n",
        "                if not pull_requests:  # No more pull requests left\n",
        "                    break\n",
        "\n",
        "                last_pr = pull_requests[-1]\n",
        "\n",
        "                merged_at_str = last_pr.get(\"merged_at\")\n",
        "                if merged_at_str:\n",
        "                    merged_at = datetime.strptime(merged_at_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                else:\n",
        "                    merged_at = None\n",
        "\n",
        "                 # Stop fetching if last pull request is before 'since' date\n",
        "\n",
        "                for pr in pull_requests:\n",
        "                    if pr.get(\"merged_at\"):\n",
        "                        pr_number = pr.get(\"number\")\n",
        "                        merged_at = datetime.strptime(pr.get(\"merged_at\"), \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                        # print(pr_number,\" \", merged_at)\n",
        "                        pr_issue_numbers.append({'pull_request_id': pr_number})\n",
        "                page += 1\n",
        "                if merged_at < since or merged_at is None:\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Failed to fetch merged pull requests: {response.status_code}\")\n",
        "                print(\"Response body:\", response.text)\n",
        "                break\n",
        "\n",
        "        return pr_issue_numbers\n",
        "\n",
        "    def get_owner_and_repo_name(self, repo_url):\n",
        "        parsed_url = urlparse(repo_url)\n",
        "        path_components = parsed_url.path.strip('/').split('/')\n",
        "        owner_username = path_components[0]\n",
        "        repo_name = path_components[1]\n",
        "        return owner_username, repo_name\n",
        "\n",
        "    def get_merge_commit_sha(self, owner, repo, pull_request_id):\n",
        "        url = f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pull_request_id}\"\n",
        "        headers = {'Authorization': f'token {self.token}'}\n",
        "        merged_commit_sha = \"\";\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            #print(response.json())\n",
        "            merge_commit_sha = response.json().get('merge_commit_sha')\n",
        "            return merge_commit_sha\n",
        "        else:\n",
        "            print(f\"Failed to fetch commits for pull request {pull_request_id}: {response.status_code}\")\n",
        "            print(\"Response body:\", response.text)\n",
        "            return None\n",
        "\n",
        "    def get_file_before(self, owner, repo, mergee_commit_sha, file_path):\n",
        "        # Get the parent commit of the specified commit\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/commits/{mergee_commit_sha}\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "        response.raise_for_status()\n",
        "        commit_info = response.json()\n",
        "\n",
        "        # Get the parent commit's tree\n",
        "        parent_sha = commit_info['parents'][0]['sha'] if commit_info['parents'] else None\n",
        "        if parent_sha:\n",
        "            # print(\"\\n\\n\\n\\n\\n\")\n",
        "            # print(parent_sha)\n",
        "            url = f\"{self.base_url}/repos/{owner}/{repo}/git/trees/{parent_sha}?recursive=1\"\n",
        "            # print(url)\n",
        "            response = requests.get(url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "            tree_info = response.json()\n",
        "\n",
        "            # Find the file in the tree\n",
        "            file_info = next((item for item in tree_info['tree'] if item['path'] == file_path),None)\n",
        "            if file_info is None:\n",
        "              return \"\"\n",
        "            # Get the content of the file\n",
        "            url = file_info['url']\n",
        "            # print(url)\n",
        "            response = requests.get(url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "            file_content = response.json()['content']\n",
        "            # print(\"\\n\\n\\n\\n\\n\")\n",
        "            formatted_code = f\"{base64.b64decode(file_content).decode('utf-8')}\"\n",
        "            # print(formatted_code)\n",
        "            # print(file_content)\n",
        "            # # Decode the Base64-encoded content\n",
        "\n",
        "            # file_content = file_content.encode('utf-8')\n",
        "            # print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "            # print(file_content)\n",
        "            # file_content = file_content.decode()\n",
        "\n",
        "            return formatted_code\n",
        "        else:\n",
        "            return \"\"\n",
        "    def get_file_after(self, owner, repo, merge_commit_sha, file_path):\n",
        "        # Get the commit information\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/commits/{merge_commit_sha}\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "        response.raise_for_status()\n",
        "        commit_info = response.json()\n",
        "\n",
        "        # Get the tree of the commit\n",
        "        tree_sha = commit_info['commit']['tree']['sha']\n",
        "        url = f\"{self.base_url}/repos/{owner}/{repo}/git/trees/{tree_sha}?recursive=1\"\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "        response.raise_for_status()\n",
        "        tree_info = response.json()\n",
        "\n",
        "        # Find the file in the tree\n",
        "        file_info = next(item for item in tree_info['tree'] if item['path'] == file_path)\n",
        "\n",
        "        # Get the content of the file\n",
        "        url = file_info['url']\n",
        "        print(f'After {url}')\n",
        "        response = requests.get(url, headers=self.headers)\n",
        "        response.raise_for_status()\n",
        "        file_content = response.json()['content']\n",
        "\n",
        "        # Format file_content as a generic code block\n",
        "        formatted_code = f\"{base64.b64decode(file_content).decode('utf-8')}\"\n",
        "\n",
        "        return formatted_code\n",
        "    def get_files(self,owner, repo, merge_commit_sha,checksum):\n",
        "        # GitHub API URL to get the commit details\n",
        "        commit_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{merge_commit_sha}\"\n",
        "\n",
        "        # Make a request to the GitHub API\n",
        "        response = requests.get(commit_url)\n",
        "        files=[]\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            commit_details = response.json()\n",
        "\n",
        "            # Check if the file exists in the commit\n",
        "            for file_info in commit_details['files']:\n",
        "                # print(\"\\n\\n\\n\\n\\n\")\n",
        "                # print(file_info)\n",
        "                # print(\"\\n\\n\\n\\n\\n\")\n",
        "                # print(file_info['filename'])\n",
        "                # print(self.get_file_before(owner,repo,merge_commit_sha,file_info['filename']))\n",
        "\n",
        "\n",
        "                files.append((file_info['filename'],self.get_file_before(owner,repo,merge_commit_sha,file_info['filename']),self.get_file_after(owner,repo,merge_commit_sha,file_info['filename']),checksum))\n",
        "                print(files)\n",
        "                checksum=checksum+1\n",
        "        # If the file was not found in the commit, return None\n",
        "        return files\n",
        "\n",
        "    def main(self, repo_url):\n",
        "        owner_username, repo_name = self.get_owner_and_repo_name(repo_url)\n",
        "        since_date = datetime.now() - timedelta(days=1000)\n",
        "        # #print(since_date)\n",
        "\n",
        "        # merged_pull_requests = self.get_merged_pull_requests(owner_username, repo_name, since_date)\n",
        "        merged_pull_requests = self.get_merged_pull_requests(owner_username, repo_name, since_date)\n",
        "\n",
        "        # print(len(merged_pull_requests))\n",
        "        # print(merged_pull_requests)\n",
        "        final_response = []\n",
        "        for merged_pr in merged_pull_requests:\n",
        "            pr_number = merged_pr.get('pull_request_id')\n",
        "            issue_number = merged_pr.get('issue_number')\n",
        "            issue_title = self.get_issue_body(owner_username, repo_name,issue_number)\n",
        "            commits = self.get_pr_commits(owner_username, repo_name,pr_number)\n",
        "            merge_commit_sha = self.get_merge_commit_sha(owner_username, repo_name,pr_number)\n",
        "            final_response.append({'pull_request_id': pr_number, 'issue_number': issue_number, 'issue_title': issue_title, 'commits_sha':commits, 'merge_commit_sha':merge_commit_sha})\n",
        "\n",
        "        #Uncomment the below print command to see the json\n",
        "        print(json.dumps(final_response, indent=4))\n",
        "\n",
        "        with open('pr_info.json', 'w') as f:\n",
        "            json.dump(final_response, f)\n",
        "        # files=self.get_filename(\"embedchain\",\"embedchain\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\",1)\n",
        "\n",
        "        # for f,x,y,c in files:\n",
        "        #   print(f+\"\\n\\n\")\n",
        "        #   print(\"Before:   \\n\")\n",
        "        #   print(x)\n",
        "        #   print(\"After:    \\n\")\n",
        "        #   print(y)\n",
        "        #   print(\"\\n\\n\\n\")\n",
        "\n",
        "repo_url = \"https://github.com/embedchain/embedchain\"\n",
        "token = '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'\n",
        "\n",
        "fetcher = GitHubInfoFetcher(token)\n",
        "fetcher.main(repo_url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHi1Z5sPLzz-"
      },
      "source": [
        "# PyDriller CommitStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g_bRp-GonXax"
      },
      "outputs": [],
      "source": [
        "from pydriller import Repository\n",
        "import subprocess\n",
        "\n",
        "class CommitStore:\n",
        "    def __init__(self, repo_url):\n",
        "        self.commit_map = {}\n",
        "        self.populate_commits(repo_url)\n",
        "\n",
        "    def populate_commits(self, repo_url):\n",
        "        repo = Repository(repo_url)\n",
        "        # print(len(self.commit_map))\n",
        "        for commit in repo.traverse_commits():\n",
        "            # print(\"Commit Hash:\", commit.hash, \"Branches:\", \", \".join(commit.branches),\"\\n\")\n",
        "            self.commit_map[commit.hash] = commit\n",
        "        print(\"map len\", len(self.commit_map))\n",
        "\n",
        "\n",
        "    def print_commit_hashes(self):\n",
        "        print(\"Number of commits: \", )\n",
        "        print(\"Commit Hashes:\")\n",
        "        for commit_hash in self.commit_map:\n",
        "              print(commit_hash)\n",
        "\n",
        "    def get_hash_and_commit(self):\n",
        "        l=[]\n",
        "        for commit_hash in self.commit_map:\n",
        "          l.append((commit_hash,self.commit_map.get(commit_hash)))\n",
        "        return l\n",
        "\n",
        "\n",
        "    def get_commit_info(self, commit_hash):\n",
        "        commit = self.commit_map.get(commit_hash)\n",
        "        # print(commit)\n",
        "        for attribute, value in commit.__dict__.items():\n",
        "            print(f\"{attribute}: {value}\")\n",
        "\n",
        "\n",
        "        if commit:\n",
        "            modified_files = commit.modified_files\n",
        "            # commit_body = commit.commit\n",
        "            message = commit.msg\n",
        "            print(message)\n",
        "            for file in modified_files:\n",
        "                print(\"FileName:\", file.filename)\n",
        "                print(\"FileDiff:\", file.diff)\n",
        "\n",
        "                for method in file.changed_methods:\n",
        "                    method_before = next((x for x in file.methods_before if x == method),\"\")\n",
        "                    method_after = next((x for x in file.methods if x == method), \"\")\n",
        "                    body_before = _getMethodBody(method_before, file.source_code_before, file)\n",
        "                    body_after = _getMethodBody(method_after, file.source_code, file)\n",
        "\n",
        "                    print(\"MethodName:\", method.name)\n",
        "                    print(\"MethodBodyBefore:\", body_before)\n",
        "                    print(\"MethodBodyAfter:\", body_after)\n",
        "                    print()\n",
        "\n",
        "        else:\n",
        "            print(f\"Commit with hash {commit_hash} not found.\")\n",
        "\n",
        "    def get_files_summarise(self, commit_hash, tree_type,issue_title=None):\n",
        "\n",
        "        commit = self.commit_map.get(commit_hash)\n",
        "        summary_gen = SummaryGenerator_Gemini()\n",
        "\n",
        "        # print(commit)\n",
        "        if commit:\n",
        "            if(issue_title==None):\n",
        "              issue_title=\"\"\n",
        "            message = commit.msg\n",
        "            modified_files = commit.modified_files\n",
        "            all_modified_methods_summaries = []\n",
        "\n",
        "            for file in modified_files:\n",
        "                # print(\"FileName:\", file.filename)\n",
        "                # print(\"FileDiff:\", file.diff)\n",
        "\n",
        "                for method in file.changed_methods:\n",
        "                    method_before = next((x for x in file.methods_before if x == method), None)\n",
        "                    method_after = next((x for x in file.methods if x == method), None)\n",
        "                    body_before = _getMethodBody(method_before, file.source_code_before, file)\n",
        "                    body_after = _getMethodBody(method_after, file.source_code, file)\n",
        "\n",
        "                    # print(\"MethodName:\", method.name)\n",
        "                    # print(\"MethodBodyBefore:\", body_before)\n",
        "                    # print(\"MethodBodyAfter:\", body_after)\n",
        "                    # print()\n",
        "                    extension = get_file_extension(file.filename)\n",
        "\n",
        "                    # Write method_before to file if not None\n",
        "                    if body_before is not None and extension:\n",
        "                        with open(f\"{method.name}_before.{extension}\", \"w\") as f:\n",
        "                            f.write(body_before)\n",
        "\n",
        "                    # Write method_after to file if not None\n",
        "                    if body_after is not None and extension:\n",
        "                        with open(f\"{method.name}_after.{extension}\", \"w\") as f:\n",
        "                            f.write(body_after)\n",
        "                    treemaker = TreeMaker(f\"{method.name}_before.{extension}\",f\"{method.name}_after.{extension}\",\"method_before\",\"method_after\",\"dot\")\n",
        "                    if(tree_type==\"ast\"):\n",
        "                      treemaker.generate_AST_trees()\n",
        "                      summary_of_method = \"\"\n",
        "                      if(issue_title):\n",
        "                        summary_of_method = \"\"+summary_gen.ast_summaries(issue_title)\n",
        "                      all_modified_methods_summaries.append(\"In file \" + file.filename + \" for the method \" + method.name + \" the change is as follows \" + summary_of_method)\n",
        "\n",
        "                    elif tree_type==\"all\":\n",
        "                      treemaker.generate_ALL_trees()\n",
        "\n",
        "                    elif tree_type==\"cdg\":\n",
        "                      treemaker.generate_CDG_trees()\n",
        "\n",
        "                    elif tree_type==\"cpg\":\n",
        "                      treemaker.generate_CPG_trees()\n",
        "\n",
        "                    elif tree_type==\"cfg\":\n",
        "                      treemaker.generate_CFG_trees()\n",
        "\n",
        "                    elif tree_type==\"ddg\":\n",
        "                      treemaker.generate_DDG_trees()\n",
        "\n",
        "                    elif tree_type==\"pdg\":\n",
        "                      treemaker.generate_PDG_trees\n",
        "\n",
        "                    subprocess.run([\"rm\", f\"{method.name}_before.{extension}\", f\"{method.name}_after.{extension}\"])\n",
        "            if(issue_title):\n",
        "              summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries, issue_title)\n",
        "            else:\n",
        "              summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries)\n",
        "\n",
        "\n",
        "            print(\"Commit Summary for the commit hash \", commit_hash,\" : \\n\")\n",
        "            print(summarise_commit)\n",
        "\n",
        "        else:\n",
        "            print(f\"Commit with hash {commit_hash} not found.\")\n",
        "        #             summary_of_method = \"\"\n",
        "        #             if(issue_title):\n",
        "        #               summary_of_method = \"\"+summary_gen.ast_summaries(issue_title)\n",
        "        #             all_modified_methods_summaries.append(\"In file \" + file.filename + \" for the method \" + method.name + \" the change is as follows \" + summary_of_method)\n",
        "        #     if(issue_title):\n",
        "        #       summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries, issue_title)\n",
        "        #     else:\n",
        "        #       summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries)\n",
        "\n",
        "\n",
        "        #     print(\"Commit Summary for the commit hash \", commit_hash,\" : \\n\")\n",
        "        #     print(summarise_commit)\n",
        "\n",
        "        # else:\n",
        "        #     print(f\"Commit with hash {commit_hash} not found.\")\n",
        "    def get_files_summarise_code(self, commit_hash,commit_msg,issue_title=None, summaries=[]):\n",
        "          summaries=[]\n",
        "          commit = self.commit_map.get(commit_hash)\n",
        "          summary_gen = SummaryGenerator_Gemini()\n",
        "          # print(commit)\n",
        "          if commit:\n",
        "              if(issue_title==None):\n",
        "                issue_title=\"\"\n",
        "              message = commit.msg\n",
        "              modified_files = commit.modified_files\n",
        "              all_modified_methods_summaries = []\n",
        "              file_summaries=[]\n",
        "              i=0\n",
        "              for file in modified_files:\n",
        "                  # print(\"FileName:\", file.filename)\n",
        "                  # print(\"FileDiff:\", file.diff)\n",
        "\n",
        "                  fileName=file.filename\n",
        "                  for method in file.changed_methods:\n",
        "\n",
        "                      method_before = next((x for x in file.methods_before if x == method), None)\n",
        "                      method_after = next((x for x in file.methods if x == method), None)\n",
        "                      body_before = _getMethodBody(method_before, file.source_code_before, file)\n",
        "                      body_after = _getMethodBody(method_after, file.source_code, file)\n",
        "\n",
        "                      # print(\"MethodName:\", method.name)\n",
        "                      # print(\"MethodBodyBefore:\", body_before)\n",
        "                      # print(\"MethodBodyAfter:\", body_after)\n",
        "                      summary_of_method = \"\"\n",
        "                      summary_of_method = \"\"+summary_gen.method_summary(body_before,body_after)\n",
        "                      # print(\"method summary :\\n\", summary_of_method)\n",
        "                      all_modified_methods_summaries.append((file.filename,method.name,summary_of_method))\n",
        "\n",
        "                  i=i+1\n",
        "                  # print(f\"gbbeuwxi 35D S{get_files(fileName,files,i)}\")\n",
        "                  # f,file_bf, file_af,c = get_files(fileName,files,i)\n",
        "                  f,file_bf, file_af = fileName,file.source_code_before,file.source_code\n",
        "                  if file_bf is None:\n",
        "                    file_bf=\"\"\n",
        "                  if file_af is None:\n",
        "                    file_af=\"\"\n",
        "                  file_summarie = \"\"+summary_gen.file_summary(fileName,file_bf,file_af,all_modified_methods_summaries)\n",
        "                  file_summaries.append((fileName,file_summarie))\n",
        "                  all_modified_methods_summaries = []\n",
        "\n",
        "\n",
        "              # if(issue_title):\n",
        "              #   summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries, issue_title)\n",
        "              # else:\n",
        "              #   summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries)\n",
        "              summarise_commit=summary_gen.commit_summary_code(commit_msg, file_summaries)\n",
        "              summaries.append({\n",
        "                  'commitId': commit_hash,\n",
        "                  'commit_message': commit_msg,\n",
        "                  'file_summaries' : file_summaries,\n",
        "                  'summary' : summarise_commit\n",
        "              })\n",
        "              # print(\"Commit Summary for the commit hash \", commit_hash,\" : \\n\")\n",
        "              # print(summarise_commit)\n",
        "              return summaries\n",
        "\n",
        "    def get_files_summarise_code_2(self, commit_hash,commit_msg,issue_title=None):\n",
        "\n",
        "      commit = self.commit_map.get(commit_hash)\n",
        "      summary_gen = SummaryGenerator_Gemini()\n",
        "      # print(commit)\n",
        "      if commit:\n",
        "          if(issue_title==None):\n",
        "            issue_title=\"\"\n",
        "          message = commit.msg\n",
        "          modified_files = commit.modified_files\n",
        "          # all_modified_methods_summaries = []\n",
        "          file_summaries=[]\n",
        "          file_diff_summaries=[]\n",
        "          i=0\n",
        "          for file in modified_files:\n",
        "              fileName=file.filename\n",
        "              f,file_bf, file_af = fileName,file.source_code_before,file.source_code\n",
        "              if file_bf is None:\n",
        "                file_bf=\"\"\n",
        "              if file_af is None:\n",
        "                file_af=\"\"\n",
        "              file_summarie = \"\"+summary_gen.file_summary_individual(fileName,file_bf,file_af)\n",
        "              file_diff_summarie=\"\"+summary_gen.file_summary_fdiff(fileName,file.diff)\n",
        "              file_summaries.append((fileName,file_summarie))\n",
        "              file_diff_summaries.append((fileName,file_diff_summarie))\n",
        "          summarise_commit=summary_gen.commit_summary_code_diff(message,file_summaries,file_diff_summaries)\n",
        "\n",
        "          # print(\"Commit Summary for the commit hash \", commit_hash,\" : \\n\")\n",
        "          # print(summarise_commit)\n",
        "          return summarise_commit\n",
        "\n",
        "\n",
        "def _getMethodBody(method, source_code, file):\n",
        "    if method and source_code:\n",
        "        lines = source_code.split(\"\\n\")\n",
        "        start = method.start_line\n",
        "        end = method.end_line\n",
        "        method_body = \"\\n\".join(lines[start - 1: end])\n",
        "\n",
        "        # Calculate the indentation level of the method\n",
        "        indentation_level = len(lines[start - 1]) - len(lines[start - 1].lstrip())\n",
        "\n",
        "        # Remove leading whitespace based on the indentation level\n",
        "        method_body_stripped = \"\\n\".join([line[indentation_level:] if len(line) >= indentation_level else '' for line in method_body.split(\"\\n\")])\n",
        "\n",
        "        return method_body_stripped\n",
        "    return None\n",
        "\n",
        "def get_files(filename,files,checksum):\n",
        "  # print(f\"fileName: {filename}   checksum: {checksum}\")\n",
        "  # print(f\"files: {files}\")\n",
        "  for f,x,y,c in files:\n",
        "    n=get_filename_from_path(f)\n",
        "    # print(f\"n: {n}   filename: {filename}  c: {c}   checksum: {checksum}\")\n",
        "    if(n==filename and c==checksum):\n",
        "      return f,x,y,c\n",
        "  return None\n",
        "\n",
        "def get_file_extension(filename):\n",
        "  # Use regex to match the file extension\n",
        "  match = re.search(r'\\.([^.]+)$', filename)\n",
        "  if match:\n",
        "      return match.group(1)\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "def get_filename_from_path(file_path):\n",
        "    # Using regular expression to extract filename with extension\n",
        "    match = re.search(r'([^/]+)$', file_path)\n",
        "    # print(f\"match {match}\")\n",
        "    # print(f\"matchjb wcyg {match.group(1)}\")\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh9PApZc7WRo"
      },
      "source": [
        "# commits sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyHhgorrHCX_",
        "outputId": "0570f58c-bb4e-4fa0-dbd1-7acbda413e92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'embedchain' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "map len 770\n",
            "[\n",
            "    {\n",
            "        \"pull_request_id\": 144,\n",
            "        \"issue_number\": \"143\",\n",
            "        \"issue_title\": \"OpenSourceApp needs openai keys\",\n",
            "        \"commits_sha\": [\n",
            "            \"fd0c44b91363455f5a3d156a9500d14ed87bde92\"\n",
            "        ],\n",
            "        \"merge_commit_sha\": \"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/embedchain/embedchain.git\n",
        "repo_url = \"./embedchain\"\n",
        "commit_store = CommitStore(repo_url)\n",
        "\n",
        "repo_url = \"https://github.com/embedchain/embedchain\"\n",
        "token = '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'\n",
        "owner = \"embedchain\"\n",
        "repo=\"embedchain\"\n",
        "fetcher = GitHubInfoFetcher(token)\n",
        "fetcher.main(repo_url)\n",
        "commit_list=commit_store.get_hash_and_commit()\n",
        "# print(commit_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "um01jaXEG8-u",
        "outputId": "ef3b184e-ebab-4732-bdb9-3364f645a1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping commit 51b49668016c57166a8bccbcf8a0392800fe8b72 with commit number 561 as it has 13 files\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 561 with commit hash a4831d6ed9351e69db0a81eca37490a9d8dba8b0\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 562 with commit hash e84b5034eae349a4ded6946210969fa3b95c08e5\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 563 with commit hash 4a5ed1dd8d7ec5f702da6e1d009b4577444afb46\n",
            "Skipping commit 51ebf3439bfdf439ca7e97aa2d5a60929ac09f44 with commit number 565 as it has 8 files\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 565 with commit hash d62a23edf6ab2c5b977280a65f536ccd8923a0d6\n",
            "Skipping commit 0ea8ab228cb622e1d877a67f6890e428840a781b with commit number 567 as it has 4 files\n",
            "Skipping commit d8897ce356700ecc319e2b981eacfa5252f754ed with commit number 568 as it has 7 files\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 568 with commit hash adde398b658197980a99a705a76279d7ac996600\n",
            "\n",
            "\n",
            "\n",
            " file: \n",
            " \n",
            "Summary generated for COMMIT NUMBER 569 with commit hash 111749a95dbd65b4edc3d920bf68d9ccab3e695c\n",
            "Skipping commit ff4a333be7ee9f41d3a0cee1e3f56c425209d15e with commit number 571 as it has 6 files\n",
            "===============================================\n"
          ]
        }
      ],
      "source": [
        "commit_hashes_issues=[]\n",
        "merge_commit_shas =[]\n",
        "commit_msgs=[]\n",
        "commit_sum=[]\n",
        "\n",
        "for merge_commit_sha,commit in commit_list:\n",
        "  merge_commit_shas.append(merge_commit_sha)\n",
        "  commit_msgs.append(commit.msg)\n",
        "checksum=1\n",
        "c=0\n",
        "summaries = []\n",
        "\n",
        "commits_summary = []\n",
        "for merge_commit_sha, commit in commit_list:\n",
        "    \n",
        "    try:\n",
        "        if c <560:\n",
        "            c += 1\n",
        "            continue\n",
        "        if c == 571:\n",
        "            break\n",
        "        if len(commit.modified_files) > 3:\n",
        "            c += 1\n",
        "            print(f\"Skipping commit {merge_commit_sha} with commit number {c} as it has {len(commit.modified_files)} files\")\n",
        "            continue\n",
        "        summary = commit_store.get_files_summarise_code_2(merge_commit_sha, commit.msg, summaries)\n",
        "\n",
        "        # Append commit hash and summary to the list\n",
        "        commits_summary.append({\"commit_hash\": merge_commit_sha, \"summary\": summary})\n",
        "\n",
        "        # Print information\n",
        "        print(f\"Summary generated for COMMIT NUMBER {c} with commit hash {merge_commit_sha}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing commit {merge_commit_sha}: {e}\")\n",
        "\n",
        "    c += 1\n",
        "\n",
        "# Write the commits_summary list to commits_summary.json\n",
        "with open(\"commits_summary.json\", \"w\") as f:\n",
        "    json.dump(commits_summary, f)\n",
        "\n",
        "\n",
        "print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5mEgS2o4k-5"
      },
      "source": [
        "# New test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Haf_chQ24ni8",
        "outputId": "e49adca8-df70-41e7-f66c-013506e050c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'embedchain' already exists and is not an empty directory.\n",
            "map len 769\n",
            "[\n",
            "    {\n",
            "        \"pull_request_id\": 144,\n",
            "        \"issue_number\": \"143\",\n",
            "        \"issue_title\": \"OpenSourceApp needs openai keys\",\n",
            "        \"commits_sha\": [\n",
            "            \"fd0c44b91363455f5a3d156a9500d14ed87bde92\"\n",
            "        ],\n",
            "        \"merge_commit_sha\": \"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/embedchain/embedchain.git\n",
        "repo_url = \"./embedchain\"\n",
        "commit_store = CommitStore(repo_url)\n",
        "\n",
        "repo_url = \"https://github.com/embedchain/embedchain\"\n",
        "token = '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'\n",
        "owner = \"embedchain\"\n",
        "repo=\"embedchain\"\n",
        "fetcher = GitHubInfoFetcher(token)\n",
        "fetcher.main(repo_url)\n",
        "# file_changes=fetcher.get_filename(\"embedchain\",\"embedchain\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\",1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "QTwsAj5A4u-2",
        "outputId": "e9d77cc3-b929-4e28-cc9d-6023070ed048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "embedchain/embedchain.py\n",
            "After https://api.github.com/repos/embedchain/embedchain/git/blobs/65ec73619245a4ae4eb7eab5b057693cb9ac0154\n",
            "[('embedchain/embedchain.py', 'import openai\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\nfrom dotenv import load_dotenv\\nfrom gpt4all import GPT4All\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom embedchain.loaders.youtube_video import YoutubeVideoLoader\\nfrom embedchain.loaders.pdf_file import PdfFileLoader\\nfrom embedchain.loaders.web_page import WebPageLoader\\nfrom embedchain.loaders.local_qna_pair import LocalQnaPairLoader\\nfrom embedchain.loaders.local_text import LocalTextLoader\\nfrom embedchain.chunkers.youtube_video import YoutubeVideoChunker\\nfrom embedchain.chunkers.pdf_file import PdfFileChunker\\nfrom embedchain.chunkers.web_page import WebPageChunker\\nfrom embedchain.chunkers.qna_pair import QnaPairChunker\\nfrom embedchain.chunkers.text import TextChunker\\nfrom embedchain.vectordb.chroma_db import ChromaDB\\n\\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\\n    organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n    model_name=\"text-embedding-ada-002\"\\n)\\nsentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\\n\\ngpt4all_model = None\\n\\nload_dotenv()\\n\\nABS_PATH = os.getcwd()\\nDB_DIR = os.path.join(ABS_PATH, \"db\")\\n\\n\\nclass EmbedChain:\\n    def __init__(self, db=None, ef=None):\\n        \"\"\"\\n        Initializes the EmbedChain instance, sets up a vector DB client and\\n        creates a collection.\\n\\n        :param db: The instance of the VectorDB subclass.\\n        \"\"\"\\n        if db is None:\\n            db = ChromaDB(ef=ef)\\n        self.db_client = db.client\\n        self.collection = db.collection\\n        self.user_asks = []\\n\\n    def _get_loader(self, data_type):\\n        \"\"\"\\n        Returns the appropriate data loader for the given data type.\\n\\n        :param data_type: The type of the data to load.\\n        :return: The loader for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        loaders = {\\n            \\'youtube_video\\': YoutubeVideoLoader(),\\n            \\'pdf_file\\': PdfFileLoader(),\\n            \\'web_page\\': WebPageLoader(),\\n            \\'qna_pair\\': LocalQnaPairLoader(),\\n            \\'text\\': LocalTextLoader(),\\n        }\\n        if data_type in loaders:\\n            return loaders[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def _get_chunker(self, data_type):\\n        \"\"\"\\n        Returns the appropriate chunker for the given data type.\\n\\n        :param data_type: The type of the data to chunk.\\n        :return: The chunker for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        chunkers = {\\n            \\'youtube_video\\': YoutubeVideoChunker(),\\n            \\'pdf_file\\': PdfFileChunker(),\\n            \\'web_page\\': WebPageChunker(),\\n            \\'qna_pair\\': QnaPairChunker(),\\n            \\'text\\': TextChunker(),\\n        }\\n        if data_type in chunkers:\\n            return chunkers[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def add(self, data_type, url):\\n        \"\"\"\\n        Adds the data from the given URL to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, url])\\n        self.load_and_embed(loader, chunker, url)\\n\\n    def add_local(self, data_type, content):\\n        \"\"\"\\n        Adds the data you supply to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param content: The local data. Refer to the `README` for formatting.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, content])\\n        self.load_and_embed(loader, chunker, content)\\n\\n    def load_and_embed(self, loader, chunker, url):\\n        \"\"\"\\n        Loads the data from the given URL, chunks it, and adds it to the database.\\n\\n        :param loader: The loader to use to load the data.\\n        :param chunker: The chunker to use to chunk the data.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        embeddings_data = chunker.create_chunks(loader, url)\\n        documents = embeddings_data[\"documents\"]\\n        metadatas = embeddings_data[\"metadatas\"]\\n        ids = embeddings_data[\"ids\"]\\n        # get existing ids, and discard doc if any common id exist.\\n        existing_docs = self.collection.get(\\n            ids=ids,\\n            # where={\"url\": url}\\n        )\\n        existing_ids = set(existing_docs[\"ids\"])\\n\\n        if len(existing_ids):\\n            data_dict = {id: (doc, meta) for id, doc, meta in zip(ids, documents, metadatas)}\\n            data_dict = {id: value for id, value in data_dict.items() if id not in existing_ids}\\n\\n            if not data_dict:\\n                print(f\"All data from {url} already exists in the database.\")\\n                return\\n\\n            ids = list(data_dict.keys())\\n            documents, metadatas = zip(*data_dict.values())\\n\\n        self.collection.add(\\n            documents=documents,\\n            metadatas=metadatas,\\n            ids=ids\\n        )\\n        print(f\"Successfully saved {url}. Total chunks count: {self.collection.count()}\")\\n\\n    def _format_result(self, results):\\n        return [\\n            (Document(page_content=result[0], metadata=result[1] or {}), result[2])\\n            for result in zip(\\n                results[\"documents\"][0],\\n                results[\"metadatas\"][0],\\n                results[\"distances\"][0],\\n            )\\n        ]\\n\\n    def get_llm_model_answer(self, prompt):\\n        raise NotImplementedError\\n\\n    def retrieve_from_database(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query\\n\\n        :param input_query: The query to use.\\n        :return: The content of the document that matched your query.\\n        \"\"\"\\n        result = self.collection.query(\\n            query_texts=[input_query,],\\n            n_results=1,\\n        )\\n        result_formatted = self._format_result(result)\\n        if result_formatted:\\n            content = result_formatted[0][0].page_content\\n        else:\\n            content = \"\"\\n        return content\\n\\n    def generate_prompt(self, input_query, context):\\n        \"\"\"\\n        Generates a prompt based on the given query and context, ready to be passed to an LLM\\n\\n        :param input_query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The prompt\\n        \"\"\"\\n        prompt = f\"\"\"Use the following pieces of context to answer the query at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n        {context}\\n        Query: {input_query}\\n        Helpful Answer:\\n        \"\"\"\\n        return prompt\\n\\n    def get_answer_from_llm(self, prompt):\\n        \"\"\"\\n        Gets an answer based on the given query and context by passing it\\n        to an LLM.\\n\\n        :param query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The answer.\\n        \"\"\"\\n        answer = self.get_llm_model_answer(prompt)\\n        return answer\\n\\n    def query(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query and then passes it to an\\n        LLM as context to get the answer.\\n\\n        :param input_query: The query to use.\\n        :return: The answer to the query.\\n        \"\"\"\\n        context = self.retrieve_from_database(input_query)\\n        prompt = self.generate_prompt(input_query, context)\\n        answer = self.get_answer_from_llm(prompt)\\n        return answer\\n\\n\\nclass App(EmbedChain):\\n    \"\"\"\\n    The EmbedChain app.\\n    Has two functions: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __int__(self, db=None, ef=None):\\n        if ef is None:\\n            ef = openai_ef\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        messages = []\\n        messages.append({\\n            \"role\": \"user\", \"content\": prompt\\n        })\\n        response = openai.ChatCompletion.create(\\n            model=\"gpt-3.5-turbo-0613\",\\n            messages=messages,\\n            temperature=0,\\n            max_tokens=1000,\\n            top_p=1,\\n        )\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass OpenSourceApp(EmbedChain):\\n    \"\"\"\\n    The OpenSource app.\\n    Same as App, but uses an open source embedding model and LLM.\\n\\n    Has two function: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __init__(self, db=None, ef=None):\\n        print(\"Loading open source embedding model. This may take some time...\")\\n        if ef is None:\\n            ef = sentence_transformer_ef\\n        print(\"Successfully loaded open source embedding model.\")\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        global gpt4all_model\\n        if gpt4all_model is None:\\n            gpt4all_model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\\n        response = gpt4all_model.generate(\\n            prompt=prompt,\\n        )\\n        return response', 'import openai\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\nfrom dotenv import load_dotenv\\nfrom gpt4all import GPT4All\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom embedchain.loaders.youtube_video import YoutubeVideoLoader\\nfrom embedchain.loaders.pdf_file import PdfFileLoader\\nfrom embedchain.loaders.web_page import WebPageLoader\\nfrom embedchain.loaders.local_qna_pair import LocalQnaPairLoader\\nfrom embedchain.loaders.local_text import LocalTextLoader\\nfrom embedchain.chunkers.youtube_video import YoutubeVideoChunker\\nfrom embedchain.chunkers.pdf_file import PdfFileChunker\\nfrom embedchain.chunkers.web_page import WebPageChunker\\nfrom embedchain.chunkers.qna_pair import QnaPairChunker\\nfrom embedchain.chunkers.text import TextChunker\\nfrom embedchain.vectordb.chroma_db import ChromaDB\\n\\n\\ngpt4all_model = None\\n\\nload_dotenv()\\n\\nABS_PATH = os.getcwd()\\nDB_DIR = os.path.join(ABS_PATH, \"db\")\\n\\n\\nclass EmbedChain:\\n    def __init__(self, db=None, ef=None):\\n        \"\"\"\\n        Initializes the EmbedChain instance, sets up a vector DB client and\\n        creates a collection.\\n\\n        :param db: The instance of the VectorDB subclass.\\n        \"\"\"\\n        if db is None:\\n            db = ChromaDB(ef=ef)\\n        self.db_client = db.client\\n        self.collection = db.collection\\n        self.user_asks = []\\n\\n    def _get_loader(self, data_type):\\n        \"\"\"\\n        Returns the appropriate data loader for the given data type.\\n\\n        :param data_type: The type of the data to load.\\n        :return: The loader for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        loaders = {\\n            \\'youtube_video\\': YoutubeVideoLoader(),\\n            \\'pdf_file\\': PdfFileLoader(),\\n            \\'web_page\\': WebPageLoader(),\\n            \\'qna_pair\\': LocalQnaPairLoader(),\\n            \\'text\\': LocalTextLoader(),\\n        }\\n        if data_type in loaders:\\n            return loaders[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def _get_chunker(self, data_type):\\n        \"\"\"\\n        Returns the appropriate chunker for the given data type.\\n\\n        :param data_type: The type of the data to chunk.\\n        :return: The chunker for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        chunkers = {\\n            \\'youtube_video\\': YoutubeVideoChunker(),\\n            \\'pdf_file\\': PdfFileChunker(),\\n            \\'web_page\\': WebPageChunker(),\\n            \\'qna_pair\\': QnaPairChunker(),\\n            \\'text\\': TextChunker(),\\n        }\\n        if data_type in chunkers:\\n            return chunkers[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def add(self, data_type, url):\\n        \"\"\"\\n        Adds the data from the given URL to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, url])\\n        self.load_and_embed(loader, chunker, url)\\n\\n    def add_local(self, data_type, content):\\n        \"\"\"\\n        Adds the data you supply to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param content: The local data. Refer to the `README` for formatting.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, content])\\n        self.load_and_embed(loader, chunker, content)\\n\\n    def load_and_embed(self, loader, chunker, url):\\n        \"\"\"\\n        Loads the data from the given URL, chunks it, and adds it to the database.\\n\\n        :param loader: The loader to use to load the data.\\n        :param chunker: The chunker to use to chunk the data.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        embeddings_data = chunker.create_chunks(loader, url)\\n        documents = embeddings_data[\"documents\"]\\n        metadatas = embeddings_data[\"metadatas\"]\\n        ids = embeddings_data[\"ids\"]\\n        # get existing ids, and discard doc if any common id exist.\\n        existing_docs = self.collection.get(\\n            ids=ids,\\n            # where={\"url\": url}\\n        )\\n        existing_ids = set(existing_docs[\"ids\"])\\n\\n        if len(existing_ids):\\n            data_dict = {id: (doc, meta) for id, doc, meta in zip(ids, documents, metadatas)}\\n            data_dict = {id: value for id, value in data_dict.items() if id not in existing_ids}\\n\\n            if not data_dict:\\n                print(f\"All data from {url} already exists in the database.\")\\n                return\\n\\n            ids = list(data_dict.keys())\\n            documents, metadatas = zip(*data_dict.values())\\n\\n        self.collection.add(\\n            documents=documents,\\n            metadatas=metadatas,\\n            ids=ids\\n        )\\n        print(f\"Successfully saved {url}. Total chunks count: {self.collection.count()}\")\\n\\n    def _format_result(self, results):\\n        return [\\n            (Document(page_content=result[0], metadata=result[1] or {}), result[2])\\n            for result in zip(\\n                results[\"documents\"][0],\\n                results[\"metadatas\"][0],\\n                results[\"distances\"][0],\\n            )\\n        ]\\n\\n    def get_llm_model_answer(self, prompt):\\n        raise NotImplementedError\\n\\n    def retrieve_from_database(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query\\n\\n        :param input_query: The query to use.\\n        :return: The content of the document that matched your query.\\n        \"\"\"\\n        result = self.collection.query(\\n            query_texts=[input_query,],\\n            n_results=1,\\n        )\\n        result_formatted = self._format_result(result)\\n        if result_formatted:\\n            content = result_formatted[0][0].page_content\\n        else:\\n            content = \"\"\\n        return content\\n\\n    def generate_prompt(self, input_query, context):\\n        \"\"\"\\n        Generates a prompt based on the given query and context, ready to be passed to an LLM\\n\\n        :param input_query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The prompt\\n        \"\"\"\\n        prompt = f\"\"\"Use the following pieces of context to answer the query at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n        {context}\\n        Query: {input_query}\\n        Helpful Answer:\\n        \"\"\"\\n        return prompt\\n\\n    def get_answer_from_llm(self, prompt):\\n        \"\"\"\\n        Gets an answer based on the given query and context by passing it\\n        to an LLM.\\n\\n        :param query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The answer.\\n        \"\"\"\\n        answer = self.get_llm_model_answer(prompt)\\n        return answer\\n\\n    def query(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query and then passes it to an\\n        LLM as context to get the answer.\\n\\n        :param input_query: The query to use.\\n        :return: The answer to the query.\\n        \"\"\"\\n        context = self.retrieve_from_database(input_query)\\n        prompt = self.generate_prompt(input_query, context)\\n        answer = self.get_answer_from_llm(prompt)\\n        return answer\\n\\n\\nclass App(EmbedChain):\\n    \"\"\"\\n    The EmbedChain app.\\n    Has two functions: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __int__(self, db=None, ef=None):\\n        if ef is None:\\n            ef = embedding_functions.OpenAIEmbeddingFunction(\\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\\n                organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n                model_name=\"text-embedding-ada-002\"\\n            )\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        messages = []\\n        messages.append({\\n            \"role\": \"user\", \"content\": prompt\\n        })\\n        response = openai.ChatCompletion.create(\\n            model=\"gpt-3.5-turbo-0613\",\\n            messages=messages,\\n            temperature=0,\\n            max_tokens=1000,\\n            top_p=1,\\n        )\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass OpenSourceApp(EmbedChain):\\n    \"\"\"\\n    The OpenSource app.\\n    Same as App, but uses an open source embedding model and LLM.\\n\\n    Has two function: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __init__(self, db=None, ef=None):\\n        print(\"Loading open source embedding model. This may take some time...\")\\n        if ef is None:\\n            ef = embedding_functions.SentenceTransformerEmbeddingFunction(\\n                model_name=\"all-MiniLM-L6-v2\"\\n            )\\n        print(\"Successfully loaded open source embedding model.\")\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        global gpt4all_model\\n        if gpt4all_model is None:\\n            gpt4all_model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\\n        response = gpt4all_model.generate(\\n            prompt=prompt,\\n        )\\n        return response', 1)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "embedchain/vectordb/chroma_db.py\n",
            "After https://api.github.com/repos/embedchain/embedchain/git/blobs/6cfa7db1201f95fb5e917633cb8e9d8ea90500b7\n",
            "[('embedchain/embedchain.py', 'import openai\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\nfrom dotenv import load_dotenv\\nfrom gpt4all import GPT4All\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom embedchain.loaders.youtube_video import YoutubeVideoLoader\\nfrom embedchain.loaders.pdf_file import PdfFileLoader\\nfrom embedchain.loaders.web_page import WebPageLoader\\nfrom embedchain.loaders.local_qna_pair import LocalQnaPairLoader\\nfrom embedchain.loaders.local_text import LocalTextLoader\\nfrom embedchain.chunkers.youtube_video import YoutubeVideoChunker\\nfrom embedchain.chunkers.pdf_file import PdfFileChunker\\nfrom embedchain.chunkers.web_page import WebPageChunker\\nfrom embedchain.chunkers.qna_pair import QnaPairChunker\\nfrom embedchain.chunkers.text import TextChunker\\nfrom embedchain.vectordb.chroma_db import ChromaDB\\n\\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\\n    organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n    model_name=\"text-embedding-ada-002\"\\n)\\nsentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\\n\\ngpt4all_model = None\\n\\nload_dotenv()\\n\\nABS_PATH = os.getcwd()\\nDB_DIR = os.path.join(ABS_PATH, \"db\")\\n\\n\\nclass EmbedChain:\\n    def __init__(self, db=None, ef=None):\\n        \"\"\"\\n        Initializes the EmbedChain instance, sets up a vector DB client and\\n        creates a collection.\\n\\n        :param db: The instance of the VectorDB subclass.\\n        \"\"\"\\n        if db is None:\\n            db = ChromaDB(ef=ef)\\n        self.db_client = db.client\\n        self.collection = db.collection\\n        self.user_asks = []\\n\\n    def _get_loader(self, data_type):\\n        \"\"\"\\n        Returns the appropriate data loader for the given data type.\\n\\n        :param data_type: The type of the data to load.\\n        :return: The loader for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        loaders = {\\n            \\'youtube_video\\': YoutubeVideoLoader(),\\n            \\'pdf_file\\': PdfFileLoader(),\\n            \\'web_page\\': WebPageLoader(),\\n            \\'qna_pair\\': LocalQnaPairLoader(),\\n            \\'text\\': LocalTextLoader(),\\n        }\\n        if data_type in loaders:\\n            return loaders[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def _get_chunker(self, data_type):\\n        \"\"\"\\n        Returns the appropriate chunker for the given data type.\\n\\n        :param data_type: The type of the data to chunk.\\n        :return: The chunker for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        chunkers = {\\n            \\'youtube_video\\': YoutubeVideoChunker(),\\n            \\'pdf_file\\': PdfFileChunker(),\\n            \\'web_page\\': WebPageChunker(),\\n            \\'qna_pair\\': QnaPairChunker(),\\n            \\'text\\': TextChunker(),\\n        }\\n        if data_type in chunkers:\\n            return chunkers[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def add(self, data_type, url):\\n        \"\"\"\\n        Adds the data from the given URL to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, url])\\n        self.load_and_embed(loader, chunker, url)\\n\\n    def add_local(self, data_type, content):\\n        \"\"\"\\n        Adds the data you supply to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param content: The local data. Refer to the `README` for formatting.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, content])\\n        self.load_and_embed(loader, chunker, content)\\n\\n    def load_and_embed(self, loader, chunker, url):\\n        \"\"\"\\n        Loads the data from the given URL, chunks it, and adds it to the database.\\n\\n        :param loader: The loader to use to load the data.\\n        :param chunker: The chunker to use to chunk the data.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        embeddings_data = chunker.create_chunks(loader, url)\\n        documents = embeddings_data[\"documents\"]\\n        metadatas = embeddings_data[\"metadatas\"]\\n        ids = embeddings_data[\"ids\"]\\n        # get existing ids, and discard doc if any common id exist.\\n        existing_docs = self.collection.get(\\n            ids=ids,\\n            # where={\"url\": url}\\n        )\\n        existing_ids = set(existing_docs[\"ids\"])\\n\\n        if len(existing_ids):\\n            data_dict = {id: (doc, meta) for id, doc, meta in zip(ids, documents, metadatas)}\\n            data_dict = {id: value for id, value in data_dict.items() if id not in existing_ids}\\n\\n            if not data_dict:\\n                print(f\"All data from {url} already exists in the database.\")\\n                return\\n\\n            ids = list(data_dict.keys())\\n            documents, metadatas = zip(*data_dict.values())\\n\\n        self.collection.add(\\n            documents=documents,\\n            metadatas=metadatas,\\n            ids=ids\\n        )\\n        print(f\"Successfully saved {url}. Total chunks count: {self.collection.count()}\")\\n\\n    def _format_result(self, results):\\n        return [\\n            (Document(page_content=result[0], metadata=result[1] or {}), result[2])\\n            for result in zip(\\n                results[\"documents\"][0],\\n                results[\"metadatas\"][0],\\n                results[\"distances\"][0],\\n            )\\n        ]\\n\\n    def get_llm_model_answer(self, prompt):\\n        raise NotImplementedError\\n\\n    def retrieve_from_database(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query\\n\\n        :param input_query: The query to use.\\n        :return: The content of the document that matched your query.\\n        \"\"\"\\n        result = self.collection.query(\\n            query_texts=[input_query,],\\n            n_results=1,\\n        )\\n        result_formatted = self._format_result(result)\\n        if result_formatted:\\n            content = result_formatted[0][0].page_content\\n        else:\\n            content = \"\"\\n        return content\\n\\n    def generate_prompt(self, input_query, context):\\n        \"\"\"\\n        Generates a prompt based on the given query and context, ready to be passed to an LLM\\n\\n        :param input_query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The prompt\\n        \"\"\"\\n        prompt = f\"\"\"Use the following pieces of context to answer the query at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n        {context}\\n        Query: {input_query}\\n        Helpful Answer:\\n        \"\"\"\\n        return prompt\\n\\n    def get_answer_from_llm(self, prompt):\\n        \"\"\"\\n        Gets an answer based on the given query and context by passing it\\n        to an LLM.\\n\\n        :param query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The answer.\\n        \"\"\"\\n        answer = self.get_llm_model_answer(prompt)\\n        return answer\\n\\n    def query(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query and then passes it to an\\n        LLM as context to get the answer.\\n\\n        :param input_query: The query to use.\\n        :return: The answer to the query.\\n        \"\"\"\\n        context = self.retrieve_from_database(input_query)\\n        prompt = self.generate_prompt(input_query, context)\\n        answer = self.get_answer_from_llm(prompt)\\n        return answer\\n\\n\\nclass App(EmbedChain):\\n    \"\"\"\\n    The EmbedChain app.\\n    Has two functions: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __int__(self, db=None, ef=None):\\n        if ef is None:\\n            ef = openai_ef\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        messages = []\\n        messages.append({\\n            \"role\": \"user\", \"content\": prompt\\n        })\\n        response = openai.ChatCompletion.create(\\n            model=\"gpt-3.5-turbo-0613\",\\n            messages=messages,\\n            temperature=0,\\n            max_tokens=1000,\\n            top_p=1,\\n        )\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass OpenSourceApp(EmbedChain):\\n    \"\"\"\\n    The OpenSource app.\\n    Same as App, but uses an open source embedding model and LLM.\\n\\n    Has two function: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __init__(self, db=None, ef=None):\\n        print(\"Loading open source embedding model. This may take some time...\")\\n        if ef is None:\\n            ef = sentence_transformer_ef\\n        print(\"Successfully loaded open source embedding model.\")\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        global gpt4all_model\\n        if gpt4all_model is None:\\n            gpt4all_model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\\n        response = gpt4all_model.generate(\\n            prompt=prompt,\\n        )\\n        return response', 'import openai\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\nfrom dotenv import load_dotenv\\nfrom gpt4all import GPT4All\\nfrom langchain.docstore.document import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom embedchain.loaders.youtube_video import YoutubeVideoLoader\\nfrom embedchain.loaders.pdf_file import PdfFileLoader\\nfrom embedchain.loaders.web_page import WebPageLoader\\nfrom embedchain.loaders.local_qna_pair import LocalQnaPairLoader\\nfrom embedchain.loaders.local_text import LocalTextLoader\\nfrom embedchain.chunkers.youtube_video import YoutubeVideoChunker\\nfrom embedchain.chunkers.pdf_file import PdfFileChunker\\nfrom embedchain.chunkers.web_page import WebPageChunker\\nfrom embedchain.chunkers.qna_pair import QnaPairChunker\\nfrom embedchain.chunkers.text import TextChunker\\nfrom embedchain.vectordb.chroma_db import ChromaDB\\n\\n\\ngpt4all_model = None\\n\\nload_dotenv()\\n\\nABS_PATH = os.getcwd()\\nDB_DIR = os.path.join(ABS_PATH, \"db\")\\n\\n\\nclass EmbedChain:\\n    def __init__(self, db=None, ef=None):\\n        \"\"\"\\n        Initializes the EmbedChain instance, sets up a vector DB client and\\n        creates a collection.\\n\\n        :param db: The instance of the VectorDB subclass.\\n        \"\"\"\\n        if db is None:\\n            db = ChromaDB(ef=ef)\\n        self.db_client = db.client\\n        self.collection = db.collection\\n        self.user_asks = []\\n\\n    def _get_loader(self, data_type):\\n        \"\"\"\\n        Returns the appropriate data loader for the given data type.\\n\\n        :param data_type: The type of the data to load.\\n        :return: The loader for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        loaders = {\\n            \\'youtube_video\\': YoutubeVideoLoader(),\\n            \\'pdf_file\\': PdfFileLoader(),\\n            \\'web_page\\': WebPageLoader(),\\n            \\'qna_pair\\': LocalQnaPairLoader(),\\n            \\'text\\': LocalTextLoader(),\\n        }\\n        if data_type in loaders:\\n            return loaders[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def _get_chunker(self, data_type):\\n        \"\"\"\\n        Returns the appropriate chunker for the given data type.\\n\\n        :param data_type: The type of the data to chunk.\\n        :return: The chunker for the given data type.\\n        :raises ValueError: If an unsupported data type is provided.\\n        \"\"\"\\n        chunkers = {\\n            \\'youtube_video\\': YoutubeVideoChunker(),\\n            \\'pdf_file\\': PdfFileChunker(),\\n            \\'web_page\\': WebPageChunker(),\\n            \\'qna_pair\\': QnaPairChunker(),\\n            \\'text\\': TextChunker(),\\n        }\\n        if data_type in chunkers:\\n            return chunkers[data_type]\\n        else:\\n            raise ValueError(f\"Unsupported data type: {data_type}\")\\n\\n    def add(self, data_type, url):\\n        \"\"\"\\n        Adds the data from the given URL to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, url])\\n        self.load_and_embed(loader, chunker, url)\\n\\n    def add_local(self, data_type, content):\\n        \"\"\"\\n        Adds the data you supply to the vector db.\\n        Loads the data, chunks it, create embedding for each chunk\\n        and then stores the embedding to vector database.\\n\\n        :param data_type: The type of the data to add.\\n        :param content: The local data. Refer to the `README` for formatting.\\n        \"\"\"\\n        loader = self._get_loader(data_type)\\n        chunker = self._get_chunker(data_type)\\n        self.user_asks.append([data_type, content])\\n        self.load_and_embed(loader, chunker, content)\\n\\n    def load_and_embed(self, loader, chunker, url):\\n        \"\"\"\\n        Loads the data from the given URL, chunks it, and adds it to the database.\\n\\n        :param loader: The loader to use to load the data.\\n        :param chunker: The chunker to use to chunk the data.\\n        :param url: The URL where the data is located.\\n        \"\"\"\\n        embeddings_data = chunker.create_chunks(loader, url)\\n        documents = embeddings_data[\"documents\"]\\n        metadatas = embeddings_data[\"metadatas\"]\\n        ids = embeddings_data[\"ids\"]\\n        # get existing ids, and discard doc if any common id exist.\\n        existing_docs = self.collection.get(\\n            ids=ids,\\n            # where={\"url\": url}\\n        )\\n        existing_ids = set(existing_docs[\"ids\"])\\n\\n        if len(existing_ids):\\n            data_dict = {id: (doc, meta) for id, doc, meta in zip(ids, documents, metadatas)}\\n            data_dict = {id: value for id, value in data_dict.items() if id not in existing_ids}\\n\\n            if not data_dict:\\n                print(f\"All data from {url} already exists in the database.\")\\n                return\\n\\n            ids = list(data_dict.keys())\\n            documents, metadatas = zip(*data_dict.values())\\n\\n        self.collection.add(\\n            documents=documents,\\n            metadatas=metadatas,\\n            ids=ids\\n        )\\n        print(f\"Successfully saved {url}. Total chunks count: {self.collection.count()}\")\\n\\n    def _format_result(self, results):\\n        return [\\n            (Document(page_content=result[0], metadata=result[1] or {}), result[2])\\n            for result in zip(\\n                results[\"documents\"][0],\\n                results[\"metadatas\"][0],\\n                results[\"distances\"][0],\\n            )\\n        ]\\n\\n    def get_llm_model_answer(self, prompt):\\n        raise NotImplementedError\\n\\n    def retrieve_from_database(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query\\n\\n        :param input_query: The query to use.\\n        :return: The content of the document that matched your query.\\n        \"\"\"\\n        result = self.collection.query(\\n            query_texts=[input_query,],\\n            n_results=1,\\n        )\\n        result_formatted = self._format_result(result)\\n        if result_formatted:\\n            content = result_formatted[0][0].page_content\\n        else:\\n            content = \"\"\\n        return content\\n\\n    def generate_prompt(self, input_query, context):\\n        \"\"\"\\n        Generates a prompt based on the given query and context, ready to be passed to an LLM\\n\\n        :param input_query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The prompt\\n        \"\"\"\\n        prompt = f\"\"\"Use the following pieces of context to answer the query at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n        {context}\\n        Query: {input_query}\\n        Helpful Answer:\\n        \"\"\"\\n        return prompt\\n\\n    def get_answer_from_llm(self, prompt):\\n        \"\"\"\\n        Gets an answer based on the given query and context by passing it\\n        to an LLM.\\n\\n        :param query: The query to use.\\n        :param context: Similar documents to the query used as context.\\n        :return: The answer.\\n        \"\"\"\\n        answer = self.get_llm_model_answer(prompt)\\n        return answer\\n\\n    def query(self, input_query):\\n        \"\"\"\\n        Queries the vector database based on the given input query.\\n        Gets relevant doc based on the query and then passes it to an\\n        LLM as context to get the answer.\\n\\n        :param input_query: The query to use.\\n        :return: The answer to the query.\\n        \"\"\"\\n        context = self.retrieve_from_database(input_query)\\n        prompt = self.generate_prompt(input_query, context)\\n        answer = self.get_answer_from_llm(prompt)\\n        return answer\\n\\n\\nclass App(EmbedChain):\\n    \"\"\"\\n    The EmbedChain app.\\n    Has two functions: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __int__(self, db=None, ef=None):\\n        if ef is None:\\n            ef = embedding_functions.OpenAIEmbeddingFunction(\\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\\n                organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n                model_name=\"text-embedding-ada-002\"\\n            )\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        messages = []\\n        messages.append({\\n            \"role\": \"user\", \"content\": prompt\\n        })\\n        response = openai.ChatCompletion.create(\\n            model=\"gpt-3.5-turbo-0613\",\\n            messages=messages,\\n            temperature=0,\\n            max_tokens=1000,\\n            top_p=1,\\n        )\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass OpenSourceApp(EmbedChain):\\n    \"\"\"\\n    The OpenSource app.\\n    Same as App, but uses an open source embedding model and LLM.\\n\\n    Has two function: add and query.\\n\\n    adds(data_type, url): adds the data from the given URL to the vector db.\\n    query(query): finds answer to the given query using vector database and LLM.\\n    \"\"\"\\n\\n    def __init__(self, db=None, ef=None):\\n        print(\"Loading open source embedding model. This may take some time...\")\\n        if ef is None:\\n            ef = embedding_functions.SentenceTransformerEmbeddingFunction(\\n                model_name=\"all-MiniLM-L6-v2\"\\n            )\\n        print(\"Successfully loaded open source embedding model.\")\\n        super().__init__(db, ef)\\n\\n    def get_llm_model_answer(self, prompt):\\n        global gpt4all_model\\n        if gpt4all_model is None:\\n            gpt4all_model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\\n        response = gpt4all_model.generate(\\n            prompt=prompt,\\n        )\\n        return response', 1), ('embedchain/vectordb/chroma_db.py', 'import chromadb\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\n\\nfrom embedchain.vectordb.base_vector_db import BaseVectorDB\\n\\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\\n    organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n    model_name=\"text-embedding-ada-002\"\\n)\\n\\nclass ChromaDB(BaseVectorDB):\\n    def __init__(self, db_dir=None, ef=None):\\n        self.ef = ef if ef is not None else openai_ef\\n        if db_dir is None:\\n            db_dir = \"db\"\\n        self.client_settings = chromadb.config.Settings(\\n            chroma_db_impl=\"duckdb+parquet\",\\n            persist_directory=db_dir,\\n            anonymized_telemetry=False\\n        )\\n        super().__init__()\\n\\n    def _get_or_create_db(self):\\n        return chromadb.Client(self.client_settings)\\n\\n    def _get_or_create_collection(self):\\n        return self.client.get_or_create_collection(\\n            \\'embedchain_store\\', embedding_function=self.ef,\\n        )\\n', 'import chromadb\\nimport os\\n\\nfrom chromadb.utils import embedding_functions\\n\\nfrom embedchain.vectordb.base_vector_db import BaseVectorDB\\n\\n\\nclass ChromaDB(BaseVectorDB):\\n    def __init__(self, db_dir=None, ef=None):\\n        if ef:\\n            self.ef = ef\\n        else:\\n            self.ef = embedding_functions.OpenAIEmbeddingFunction(\\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\\n                organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\\n                model_name=\"text-embedding-ada-002\"\\n            )\\n        if db_dir is None:\\n            db_dir = \"db\"\\n        self.client_settings = chromadb.config.Settings(\\n            chroma_db_impl=\"duckdb+parquet\",\\n            persist_directory=db_dir,\\n            anonymized_telemetry=False\\n        )\\n        super().__init__()\\n\\n    def _get_or_create_db(self):\\n        return chromadb.Client(self.client_settings)\\n\\n    def _get_or_create_collection(self):\\n        return self.client.get_or_create_collection(\\n            \\'embedchain_store\\', embedding_function=self.ef,\\n        )\\n', 2)]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "CommitStore.get_files_summarise_code() missing 1 required positional argument: 'files'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ca69994f68a0>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# commit_store.get_commit_info(merge_commit_sha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_commit_sha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_files_summarise_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_commit_sha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchecksum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CommitStore.get_files_summarise_code() missing 1 required positional argument: 'files'"
          ]
        }
      ],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "with open('pr_info.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "merge_commit_shas = []\n",
        "for pr_json in data:\n",
        "  merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "  commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "# print(\"mcs , issue title:\")\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "checksum=1\n",
        "for merge_commit_sha, issue_title in zip(merge_commit_shas, commit_hashes_issues):\n",
        "    # print(f\"Commit hash: {commit_hash}\")\n",
        "    # commit_store.get_commit_info(merge_commit_sha)\n",
        "    files=fetcher.get_files(owner,repo,merge_commit_sha,checksum)\n",
        "    summary = commit_store.get_files_summarise_code(merge_commit_sha,files)\n",
        "    checksum = checksum+1\n",
        "\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "IOQTrqMqYTuL",
        "outputId": "a476393c-6af9-46f8-9c5c-66475a6fbd08"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "list.append() takes exactly one argument (0 given)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-883f15200e42>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mcommit_hashes_issues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'issue_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmerge_commit_shas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# print(\"mcs , issue title:\")2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (0 given)"
          ]
        }
      ],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "with open('pr_info.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "merge_commit_shas = []\n",
        "for pr_json in data:\n",
        "  merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "  commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "merge_commit_shas.append()\n",
        "\n",
        "# print(\"mcs , issue title:\")2\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "checksum=1\n",
        "for merge_commit_sha, issue_title in zip(merge_commit_shas, commit_hashes_issues):\n",
        "    # print(f\"Commit hash: {commit_hash}\")\n",
        "    # commit_store.get_commit_info(merge_commit_sha)\n",
        "    files=fetcher.get_files(owner,repo,merge_commit_sha,checksum)\n",
        "    summary = commit_store.get_files_summarise_code_2(merge_commit_sha,files)\n",
        "    checksum = checksum+1\n",
        "\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebsTXNN_4RIW"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KReNzvddvLdn",
        "outputId": "ebc8000b-dde8-4463-85b7-6e656442ae7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'embedchain' already exists and is not an empty directory.\n",
            "map len 769\n",
            "[\n",
            "    {\n",
            "        \"pull_request_id\": 144,\n",
            "        \"issue_number\": \"143\",\n",
            "        \"issue_title\": \"OpenSourceApp needs openai keys\",\n",
            "        \"commits_sha\": [\n",
            "            \"fd0c44b91363455f5a3d156a9500d14ed87bde92\"\n",
            "        ],\n",
            "        \"merge_commit_sha\": \"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/embedchain/embedchain.git\n",
        "repo_url = \"./embedchain\"\n",
        "commit_store = CommitStore(repo_url)\n",
        "\n",
        "repo_url = \"https://github.com/embedchain/embedchain\"\n",
        "token = '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'\n",
        "\n",
        "fetcher = GitHubInfoFetcher(token)\n",
        "fetcher.main(repo_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASl56Ksw2H10"
      },
      "outputs": [],
      "source": [
        "\n",
        "treemaker = TreeMaker(f\"fu_before.py\",f\"fu_after.py\",\"method_before\",\"method_after\",\"dot\")\n",
        "treemaker.generate_ALL_trees()\n",
        "treemaker.generate_AST_trees()\n",
        "treemaker.generate_CPG_trees()\n",
        "summary_gen = SummaryGenerator_Gemini()\n",
        "summary_gen.ast_summaries()\n",
        "\n",
        "\n",
        "                    # if(tree_type==\"ast\"):\n",
        "                    #   treemaker.generate_AST_trees()\n",
        "                    #   summary_of_method = \"\"\n",
        "                    #   if(issue_title):\n",
        "                    #     summary_of_method = \"\"+summary_gen.ast_summaries(issue_title)\n",
        "                    #   all_modified_methods_summaries.append(\"In file \" + file.filename + \" for the method \" + method.name + \" the change is as follows \" + summary_of_method)\n",
        "\n",
        "                    # elif tree_type==\"all\":\n",
        "                    #   treemaker.generate_ALL_trees()\n",
        "\n",
        "                    # elif tree_type==\"cdg\":\n",
        "                    #   treemaker.generate_CDG_trees()\n",
        "\n",
        "                    # elif tree_type==\"cpg\":\n",
        "                    #   treemaker.generate_CPG_trees()\n",
        "\n",
        "                    # elif tree_type==\"cfg\":\n",
        "                    #   treemaker.generate_CFG_trees()\n",
        "\n",
        "                    # elif tree_type==\"ddg\":\n",
        "                    #   treemaker.generate_DDG_trees()\n",
        "\n",
        "                    # elif tree_type==\"pdg\":\n",
        "                    #   treemaker.generate_PDG_trees\n",
        "\n",
        "                    # subprocess.run([\"rm\", f\"{method.name}_before.{extension}\", f\"{method.name}_after.{extension}\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtZMveYmL_TA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79jUSFwTGmYw"
      },
      "outputs": [],
      "source": [
        "\n",
        "treemaker = TreeMaker(f\"fu_before.py\",f\"fu_after.py\",\"method_before\",\"method_after\",\"dot\")\n",
        "treemaker.generate_ALL_trees()\n",
        "summary_gen = SummaryGenerator_Gemini()\n",
        "summary_gen.all_summaries()\n",
        "\n",
        "\n",
        "                    # if(tree_type==\"ast\"):\n",
        "                    #   treemaker.generate_AST_trees()\n",
        "                    #   summary_of_method = \"\"\n",
        "                    #   if(issue_title):\n",
        "                    #     summary_of_method = \"\"+summary_gen.ast_summaries(issue_title)\n",
        "                    #   all_modified_methods_summaries.append(\"In file \" + file.filename + \" for the method \" + method.name + \" the change is as follows \" + summary_of_method)\n",
        "\n",
        "                    # elif tree_type==\"all\":\n",
        "                    #   treemaker.generate_ALL_trees()\n",
        "\n",
        "                    # elif tree_type==\"cdg\":\n",
        "                    #   treemaker.generate_CDG_trees()\n",
        "\n",
        "                    # elif tree_type==\"cpg\":\n",
        "                    #   treemaker.generate_CPG_trees()\n",
        "\n",
        "                    # elif tree_type==\"cfg\":\n",
        "                    #   treemaker.generate_CFG_trees()\n",
        "\n",
        "                    # elif tree_type==\"ddg\":\n",
        "                    #   treemaker.generate_DDG_trees()\n",
        "\n",
        "                    # elif tree_type==\"pdg\":\n",
        "                    #   treemaker.generate_PDG_trees\n",
        "\n",
        "                    # subprocess.run([\"rm\", f\"{method.name}_before.{extension}\", f\"{method.name}_after.{extension}\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6r3VP7oGwY8"
      },
      "outputs": [],
      "source": [
        "\n",
        "treemaker = TreeMaker(f\"fu_before.py\",f\"fu_after.py\",\"method_before\",\"method_after\",\"dot\")\n",
        "treemaker.generate_CPG_trees()\n",
        "summary_gen = SummaryGenerator_Gemini()\n",
        "summary_gen.cpg_summaries()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiBgFzY9vEi3"
      },
      "outputs": [],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "with open('pr_info.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "merge_commit_shas = []\n",
        "for pr_json in data:\n",
        "  merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "  commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "# print(\"mcs , issue title:\")\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "for merge_commit_sha, issue_title in zip(merge_commit_shas, commit_hashes_issues):\n",
        "    # print(f\"Commit hash: {commit_hash}\")\n",
        "    commit_store.get_commit_info(merge_commit_sha)\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQxg72-Evini"
      },
      "outputs": [],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "with open('pr_info.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "merge_commit_shas = []\n",
        "for pr_json in data:\n",
        "  merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "  commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "# print(\"mcs , issue title:\")\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "for merge_commit_sha, issue_title in zip(merge_commit_shas, commit_hashes_issues):\n",
        "    # print(f\"Commit hash: {commit_hash}\")\n",
        "    commit_store.get_files_summarise(merge_commit_sha,\"ast\",issue_title)\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bicuuplgrpw2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "mY2davuL7s1n",
        "outputId": "f8de58ec-84c5-4f91-b52d-b59c786bb7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cccccccccccc 1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md\n",
            "After https://api.github.com/repos/embedchain/embedchain/git/blobs/89cdf64a9a204b21dc1a38d01eeacf10e67aee31\n",
            "[('README.md', '', '# embedchain', 1)]\n",
            "jkeffffffffff [('README.md', '', '# embedchain', 1)]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-66f65e7216b7>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"jkeffffffffff {files}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_files_summarise_code_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_commit_sha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mcommit_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-bb49acb36f54>\u001b[0m in \u001b[0;36mget_files_summarise_code_2\u001b[0;34m(self, commit_hash, commit_msg, files, issue_title)\u001b[0m\n\u001b[1;32m    247\u001b[0m               \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;31m#   summarise_commit = summary_gen.commit_summary(all_modified_methods_summaries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m               \u001b[0msummarise_commit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit_summary_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Commit Summary for the commit hash \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" : \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f2a63e2454d3>\u001b[0m in \u001b[0;36mcommit_summary_code\u001b[0;34m(self, commit_msg, file_summaries)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nIn this file\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" the changes have been summarized like this : \\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n Commit message : \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcommit_msg\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" don't give seperated file summaries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mcommit_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# print(\"Commit summary: \", commit_summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, request_options)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             response = getattr(self._session, method)(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    542\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "merge_commit_shas =[]\n",
        "commit_msgs=[]\n",
        "commit_sum=[]\n",
        "\n",
        "for merge_commit_sha,commit in commit_list:\n",
        "  merge_commit_shas.append(merge_commit_sha)\n",
        "  commit_msgs.append(commit.msg)\n",
        "# with open('pr_info.json', 'r') as file:\n",
        "#     data = json.load(file)\n",
        "# merge_commit_shas = []\n",
        "# for pr_json in data:\n",
        "#   merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "#   commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "# print(\"mcs , issue title:\")\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "checksum=1\n",
        "c=0\n",
        "for merge_commit_sha, commit in commit_list:\n",
        "    # print(f\"Commit hash: {commit_hash}\")\n",
        "    # commit_store.get_commit_info(merge_commit_sha)\n",
        "    if(c==10):\n",
        "      break\n",
        "    print(f\"cccccccccccc {checksum}\")\n",
        "    files=fetcher.get_files(owner,repo,merge_commit_sha,checksum)\n",
        "    if files is None:\n",
        "      # checksum = checksum-1\n",
        "      continue\n",
        "    print(f\"jkeffffffffff {files}\")\n",
        "    summary = commit_store.get_files_summarise_code_2(merge_commit_sha,commit.msg,files)\n",
        "    commit_sum.append(summary[:-1])\n",
        "    c=c+1\n",
        "    # checksum = checksum+1\n",
        "\n",
        "    print(\"===============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAG7JpDv_71B"
      },
      "outputs": [],
      "source": [
        "# commit_hashes = [\"6fcc33ec4ec075090adc292368ece5afc30e9995\",\"fd0c44b91363455f5a3d156a9500d14ed87bde92\",\"54287653298e18ef5661e3224958de0bb06d5864\",\"200f11a0e0590c554a0379e40c94e82a0da7ce7c\"]\n",
        "\n",
        "commit_hashes_issues=[]\n",
        "merge_commit_shas =[]\n",
        "commit_msgs=[]\n",
        "commit_sum=[]\n",
        "\n",
        "for merge_commit_sha,commit in commit_list:\n",
        "  merge_commit_shas.append(merge_commit_sha)\n",
        "  commit_msgs.append(commit.msg)\n",
        "  for f in commit.modified_files:\n",
        "    print(f.source_code_before)\n",
        "# with open('pr_info.json', 'r') as file:\n",
        "#     data = json.load(file)\n",
        "# merge_commit_shas = []\n",
        "# for pr_json in data:\n",
        "#   merge_commit_shas.append(pr_json['merge_commit_sha'])\n",
        "#   commit_hashes_issues.append(pr_json['issue_title'])\n",
        "\n",
        "# print(\"mcs , issue title:\")\n",
        "# for sha in merge_commit_shas:\n",
        "#     print(sha)\n",
        "#     print(commit_hashes_issues[merge_commit_shas.index(sha)])\n",
        "checksum=1\n",
        "c=0\n",
        "summaries = []\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JcgSRMTaLoRZ",
        "m5mEgS2o4k-5",
        "ebsTXNN_4RIW"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
